{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "989a8be4-6741-4cb6-9ce9-d5060f0bac08",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /databricks/python3/lib/python3.10/site-packages (2.28.1)\r\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.10/site-packages (from requests) (1.26.11)\r\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.10/site-packages (from requests) (2022.9.14)\r\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.10/site-packages (from requests) (3.3)\r\nRequirement already satisfied: charset-normalizer<3,>=2 in /databricks/python3/lib/python3.10/site-packages (from requests) (2.0.4)\r\n\r\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.2.2\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.3.1\u001B[0m\r\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b6995cc-6949-43a0-a1b9-81e8b296d56e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Retrieved Successfully:\n{'BEAAPI': {'Request': {'RequestParam': [{'ParameterName': 'USERID', 'ParameterValue': '722F4B0C-A996-459A-9E2D-6FF6393FB1DC'}, {'ParameterName': 'METHOD', 'ParameterValue': 'GETDATASETLIST'}, {'ParameterName': 'RESULTFORMAT', 'ParameterValue': 'JSON'}]}, 'Results': {'Dataset': [{'DatasetName': 'NIPA', 'DatasetDescription': 'Standard NIPA tables'}, {'DatasetName': 'NIUnderlyingDetail', 'DatasetDescription': 'Standard NI underlying detail tables'}, {'DatasetName': 'MNE', 'DatasetDescription': 'Multinational Enterprises'}, {'DatasetName': 'FixedAssets', 'DatasetDescription': 'Standard Fixed Assets tables'}, {'DatasetName': 'ITA', 'DatasetDescription': 'International Transactions Accounts'}, {'DatasetName': 'IIP', 'DatasetDescription': 'International Investment Position'}, {'DatasetName': 'InputOutput', 'DatasetDescription': 'Input-Output Data'}, {'DatasetName': 'IntlServTrade', 'DatasetDescription': 'International Services Trade'}, {'DatasetName': 'IntlServSTA', 'DatasetDescription': 'International Services Supplied Through Affiliates'}, {'DatasetName': 'GDPbyIndustry', 'DatasetDescription': 'GDP by Industry'}, {'DatasetName': 'Regional', 'DatasetDescription': 'Regional data sets'}, {'DatasetName': 'UnderlyingGDPbyIndustry', 'DatasetDescription': 'Underlying GDP by Industry'}, {'DatasetName': 'APIDatasetMetaData', 'DatasetDescription': 'Metadata about other API datasets'}]}}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Replace 'Your-36Character-Key' with your actual BEA API key\n",
    "api_key = '722F4B0C-A996-459A-9E2D-6FF6393FB1DC'\n",
    "\n",
    "# Set up the API endpoint and parameters\n",
    "api_endpoint = 'https://apps.bea.gov/api/data'\n",
    "params = {\n",
    "    'UserID': api_key,\n",
    "    'method': 'GETDATASETLIST',  # Example method, change as needed\n",
    "    'ResultFormat': 'JSON'\n",
    "}\n",
    "\n",
    "# Make the GET request\n",
    "response = requests.get(api_endpoint, params=params)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the JSON response\n",
    "    data = response.json()\n",
    "    print(\"Data Retrieved Successfully:\")\n",
    "    print(data)\n",
    "else:\n",
    "    print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d3e5d60-a66f-4c73-b0c6-a7486f11d02a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter List Retrieved Successfully:\n{'BEAAPI': {'Request': {'RequestParam': [{'ParameterName': 'USERID', 'ParameterValue': '722F4B0C-A996-459A-9E2D-6FF6393FB1DC'}, {'ParameterName': 'METHOD', 'ParameterValue': 'GETPARAMETERLIST'}, {'ParameterName': 'DATASETNAME', 'ParameterValue': 'NIPA'}, {'ParameterName': 'RESULTFORMAT', 'ParameterValue': 'JSON'}]}, 'Results': {'Parameter': [{'ParameterName': 'Frequency', 'ParameterDataType': 'string', 'ParameterDescription': 'A - Annual, Q-Quarterly, M-Monthly', 'ParameterIsRequiredFlag': '1', 'ParameterDefaultValue': '', 'MultipleAcceptedFlag': '1', 'AllValue': ''}, {'ParameterName': 'ShowMillions', 'ParameterDataType': 'string', 'ParameterDescription': 'A flag indicating that million-dollar data should be returned.', 'ParameterIsRequiredFlag': '0', 'ParameterDefaultValue': 'N', 'MultipleAcceptedFlag': '0', 'AllValue': ''}, {'ParameterName': 'TableID', 'ParameterDataType': 'integer', 'ParameterDescription': 'The standard NIPA table identifier', 'ParameterIsRequiredFlag': '0', 'MultipleAcceptedFlag': '0', 'AllValue': ''}, {'ParameterName': 'TableName', 'ParameterDataType': 'string', 'ParameterDescription': 'The new NIPA table identifier', 'ParameterIsRequiredFlag': '0', 'MultipleAcceptedFlag': '0', 'AllValue': ''}, {'ParameterName': 'Year', 'ParameterDataType': 'integer', 'ParameterDescription': 'List of year(s) of data to retrieve (X for All)', 'ParameterIsRequiredFlag': '1', 'ParameterDefaultValue': '', 'MultipleAcceptedFlag': '1', 'AllValue': 'X'}]}}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "api_key = '722F4B0C-A996-459A-9E2D-6FF6393FB1DC'\n",
    "api_endpoint = 'https://apps.bea.gov/api/data'\n",
    "params = {\n",
    "    'UserID': api_key,\n",
    "    'method': 'GETPARAMETERLIST',\n",
    "    'datasetname': 'NIPA',  # Change to your dataset of interest\n",
    "    'ResultFormat': 'JSON'\n",
    "}\n",
    "\n",
    "response = requests.get(api_endpoint, params=params)\n",
    "if response.status_code == 200:\n",
    "    parameter_list = response.json()\n",
    "    print(\"Parameter List Retrieved Successfully:\")\n",
    "    print(parameter_list)\n",
    "else:\n",
    "    print(f\"Failed to retrieve parameter list. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01558792-9ec1-48bd-aa8e-8284d7764703",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the DataFrame:\n['ParameterName', 'ParameterDataType', 'ParameterDescription', 'ParameterIsRequiredFlag', 'ParameterDefaultValue', 'MultipleAcceptedFlag', 'AllValue']\n\nSample Rows:\n  ParameterName ParameterDataType  \\\n0     Frequency            string   \n1  ShowMillions            string   \n2       TableID           integer   \n3     TableName            string   \n4          Year           integer   \n\n                                ParameterDescription ParameterIsRequiredFlag  \\\n0                 A - Annual, Q-Quarterly, M-Monthly                       1   \n1  A flag indicating that million-dollar data sho...                       0   \n2                 The standard NIPA table identifier                       0   \n3                      The new NIPA table identifier                       0   \n4    List of year(s) of data to retrieve (X for All)                       1   \n\n  ParameterDefaultValue MultipleAcceptedFlag AllValue  \n0                                          1           \n1                     N                    0           \n2                   NaN                    0           \n3                   NaN                    0           \n4                                          1        X  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "# Replace with your actual BEA API key\n",
    "api_key = '722F4B0C-A996-459A-9E2D-6FF6393FB1DC'\n",
    "\n",
    "# Set up the API endpoint and parameters\n",
    "api_endpoint = 'https://apps.bea.gov/api/data'\n",
    "params = {\n",
    "    'UserID': api_key,\n",
    "    'method': 'GETPARAMETERLIST',\n",
    "    'datasetname': 'NIPA',  # Example dataset\n",
    "    'ResultFormat': 'JSON'\n",
    "}\n",
    "\n",
    "# Make the GET request to the API\n",
    "response = requests.get(api_endpoint, params=params)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the JSON response\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract the 'Parameter' list\n",
    "    parameters = data['BEAAPI']['Results']['Parameter']\n",
    "\n",
    "    # Convert the list to a DataFrame\n",
    "    df = pd.DataFrame(parameters)\n",
    "\n",
    "    # Display the column names\n",
    "    print(\"Columns in the DataFrame:\")\n",
    "    print(df.columns.tolist())\n",
    "\n",
    "    # Display a few sample rows (top 5 rows)\n",
    "    print(\"\\nSample Rows:\")\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73f02019-2b85-4cb7-a1db-80410d663084",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-792501268012455>, line 42\u001B[0m\n",
       "\u001B[1;32m     39\u001B[0m data \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mjson()\n",
       "\u001B[1;32m     41\u001B[0m \u001B[38;5;66;03m# Extract table and line code pairs\u001B[39;00m\n",
       "\u001B[0;32m---> 42\u001B[0m all_pairs \u001B[38;5;241m=\u001B[39m [(item[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTableName\u001B[39m\u001B[38;5;124m'\u001B[39m], item[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLineCode\u001B[39m\u001B[38;5;124m'\u001B[39m]) \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBEAAPI\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mResults\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mParamValue\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m item[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTableName\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;129;01min\u001B[39;00m required_tables]\n",
       "\u001B[1;32m     44\u001B[0m \u001B[38;5;66;03m# Iterate over each pair and make API calls\u001B[39;00m\n",
       "\u001B[1;32m     45\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m table_name, line_code \u001B[38;5;129;01min\u001B[39;00m all_pairs:\n",
       "\n",
       "File \u001B[0;32m<command-792501268012455>, line 42\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n",
       "\u001B[1;32m     39\u001B[0m data \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mjson()\n",
       "\u001B[1;32m     41\u001B[0m \u001B[38;5;66;03m# Extract table and line code pairs\u001B[39;00m\n",
       "\u001B[0;32m---> 42\u001B[0m all_pairs \u001B[38;5;241m=\u001B[39m [(item[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTableName\u001B[39m\u001B[38;5;124m'\u001B[39m], item[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLineCode\u001B[39m\u001B[38;5;124m'\u001B[39m]) \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBEAAPI\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mResults\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mParamValue\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mitem\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mTableName\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m \u001B[38;5;129;01min\u001B[39;00m required_tables]\n",
       "\u001B[1;32m     44\u001B[0m \u001B[38;5;66;03m# Iterate over each pair and make API calls\u001B[39;00m\n",
       "\u001B[1;32m     45\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m table_name, line_code \u001B[38;5;129;01min\u001B[39;00m all_pairs:\n",
       "\n",
       "\u001B[0;31mKeyError\u001B[0m: 'TableName'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)\nFile \u001B[0;32m<command-792501268012455>, line 42\u001B[0m\n\u001B[1;32m     39\u001B[0m data \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mjson()\n\u001B[1;32m     41\u001B[0m \u001B[38;5;66;03m# Extract table and line code pairs\u001B[39;00m\n\u001B[0;32m---> 42\u001B[0m all_pairs \u001B[38;5;241m=\u001B[39m [(item[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTableName\u001B[39m\u001B[38;5;124m'\u001B[39m], item[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLineCode\u001B[39m\u001B[38;5;124m'\u001B[39m]) \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBEAAPI\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mResults\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mParamValue\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m item[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTableName\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;129;01min\u001B[39;00m required_tables]\n\u001B[1;32m     44\u001B[0m \u001B[38;5;66;03m# Iterate over each pair and make API calls\u001B[39;00m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m table_name, line_code \u001B[38;5;129;01min\u001B[39;00m all_pairs:\n\nFile \u001B[0;32m<command-792501268012455>, line 42\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     39\u001B[0m data \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mjson()\n\u001B[1;32m     41\u001B[0m \u001B[38;5;66;03m# Extract table and line code pairs\u001B[39;00m\n\u001B[0;32m---> 42\u001B[0m all_pairs \u001B[38;5;241m=\u001B[39m [(item[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTableName\u001B[39m\u001B[38;5;124m'\u001B[39m], item[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLineCode\u001B[39m\u001B[38;5;124m'\u001B[39m]) \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBEAAPI\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mResults\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mParamValue\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mitem\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mTableName\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m \u001B[38;5;129;01min\u001B[39;00m required_tables]\n\u001B[1;32m     44\u001B[0m \u001B[38;5;66;03m# Iterate over each pair and make API calls\u001B[39;00m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m table_name, line_code \u001B[38;5;129;01min\u001B[39;00m all_pairs:\n\n\u001B[0;31mKeyError\u001B[0m: 'TableName'",
       "errorSummary": "<span class='ansi-red-fg'>KeyError</span>: 'TableName'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Function to construct the API URL\n",
    "def construct_api_url(api_key, table_name, line_code, geo_fips, year='ALL'):\n",
    "    base_url = \"https://apps.bea.gov/api/data/\"\n",
    "    params = {\n",
    "        'method': 'GetData',\n",
    "        'UserID': api_key,\n",
    "        'datasetname': 'Regional',\n",
    "        'Year': year,\n",
    "        'GeoFips': geo_fips,\n",
    "        'TableName': table_name,\n",
    "        'LineCode': line_code\n",
    "    }\n",
    "    return base_url + '?' + '&'.join([f'{key}={value}' for key, value in params.items()])\n",
    "\n",
    "# Define the mapping of table prefixes to GeoFips values\n",
    "geo_fips_mapping = {\n",
    "    'S': 'STATE',\n",
    "    'M': 'MSA',\n",
    "    'P': 'PORT',\n",
    "    'C': ['COUNTY', 'MSA', 'MIC']  # 'C' tables can have multiple GeoFips values\n",
    "}\n",
    "\n",
    "# List of required tables for Gold\n",
    "required_tables = [\n",
    "    'CAGDP1', 'CAGDP2', 'CAGDP8', 'CAGDP9', 'CAGDP11'\n",
    "]\n",
    "\n",
    "# API key\n",
    "api_key = '722F4B0C-A996-459A-9E2D-6FF6393FB1DC' \n",
    "\n",
    "# Fetch TableName and LineCode pairs\n",
    "\n",
    "fetch_url = f\"https://apps.bea.gov/api/data/?UserID={api_key}&method=GetParameterValuesFiltered&datasetname=Regional&TargetParameter=LineCode\"\n",
    "response = requests.get(fetch_url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract table and line code pairs\n",
    "    all_pairs = [(item['TableName'], item['LineCode']) for item in data['BEAAPI']['Results']['ParamValue'] if item['TableName'] in required_tables]\n",
    "\n",
    "    # Iterate over each pair and make API calls\n",
    "    for table_name, line_code in all_pairs:\n",
    "        geo_fips_values = geo_fips_mapping.get(table_name[0], 'STATE')  # Default to 'STATE' if prefix not in mapping\n",
    "\n",
    "        # Handle cases where multiple GeoFips values are possible\n",
    "        if isinstance(geo_fips_values, list):\n",
    "            for geo_fips in geo_fips_values:\n",
    "                api_url = construct_api_url(api_key, table_name, line_code, geo_fips)\n",
    "                # Make the API call and handle the response...\n",
    "        else:\n",
    "            api_url = construct_api_url(api_key, table_name, line_code, geo_fips_values)\n",
    "            # Make the API call and handle the response...\n",
    "\n",
    "        # Example of handling the response\n",
    "        response = requests.get(api_url)\n",
    "        if response.status_code == 200:\n",
    "            # Process the data as needed\n",
    "            print(f\"Data for {table_name}, LineCode {line_code}, GeoFips {geo_fips}: {response.json()}\")\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data for {table_name}, LineCode {line_code}. Status code: {response.status_code}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to retrieve TableName and LineCode pairs. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "587cac78-47d6-450a-b28d-e453d4f331e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests as r\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "import ast\n",
    "URL='https://apps.bea.gov/api/data'\n",
    "api_key='722F4B0C-A996-459A-9E2D-6FF6393FB1DC'\n",
    "res=r.get('''https://apps.bea.gov/api/data/?UserID=api_key&method=GetParameterValuesFiltered&datasetname=Regional&TargetParameter=LineCode&TableName=CAGDP2&ResultFormat=json''')\n",
    "line_codes=['1', '10', '11', '12', '13', '2', '25', '3', '34', '35', '36', '45', '50', '51', '56', '59', '6', '60', '64', '65', '68', '69', '70', '75', '76', '79', '82', '83', '87', '88', '89', '90', '91', '92']\n",
    "#for codeno in res['BEAAPI']['Results']['ParamValue']:\n",
    "#    code.append(codeno['Key'])\n",
    "#print(code)\n",
    "for code in line_codes:\n",
    "    f_url = URL+'?&'+'UserID='+f'{api_key}'+'&method=GETDATA'+'&datasetname=Regional'+'&TableName=CAGDP2'+'&GeoFips=MSA'+'&Year=ALL'+f'&LineCode={code}'+'&ResultFormat=JSON'\n",
    "    res = r.get(f_url).text\n",
    "    res=ast.literal_eval(res)\n",
    "    df=spark.createDataFrame(res['BEAAPI']['Results']['Data'])\n",
    "    df.createOrReplaceTempView(f'CAGDP2_LineCode_{code}')\n",
    "    #spark.sql(f'''create or replace table CAGDP2_LineCode_{code} As select * from CAGDP2_LineCode_{code} ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "729cddf7-58c3-4454-89fb-e8dc597223fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[CL_UNIT: string, Code: string, DataValue: string, GeoFips: string, GeoName: string, TimePeriod: string, UNIT_MULT: string, NoteRef: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from CAGDP2_LineCode_11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "955df4ab-a691-4af7-bee1-380d7494c499",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of Distinct LineCode Values for Table CAGDP9:\n['1', '10', '11', '12', '13', '2', '25', '3', '34', '35', '36', '45', '50', '51', '56', '59', '6', '60', '64', '65', '68', '69', '70', '75', '76', '79', '82', '83', '87', '88', '89', '90', '91', '92']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Replace with your actual BEA API key\n",
    "api_key = '722F4B0C-A996-459A-9E2D-6FF6393FB1DC'\n",
    "\n",
    "# Set up the API endpoint and parameters for fetching LineCode values\n",
    "api_endpoint = 'https://apps.bea.gov/api/data'\n",
    "params = {\n",
    "    'UserID': api_key,\n",
    "    'method': 'GetParameterValuesFiltered',\n",
    "    'datasetname': 'Regional',\n",
    "    'TableName': 'CAGDP11',\n",
    "    'TargetParameter': 'LineCode',\n",
    "    'ResultFormat': 'JSON'\n",
    "}\n",
    "\n",
    "# Make the GET request to the API\n",
    "response = requests.get(api_endpoint, params=params)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the JSON response\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract and display the LineCode values\n",
    "    line_codes = [item['Key'] for item in data['BEAAPI']['Results']['ParamValue']]\n",
    "    print(\"List of Distinct LineCode Values for Table CAGDP9:\")\n",
    "    print(line_codes)\n",
    "else:\n",
    "    print(f\"Failed to retrieve LineCode values for Table CAGDP2. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61f240a9-cb97-4d8c-9c96-d05c0a06d3df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record count for LineCode 1: 385\nNumber of columns for LineCode 1: 7\nRecord count for LineCode 10: 385\nNumber of columns for LineCode 10: 8\nRecord count for LineCode 11: 385\nNumber of columns for LineCode 11: 8\nRecord count for LineCode 12: 385\nNumber of columns for LineCode 12: 8\nRecord count for LineCode 13: 385\nNumber of columns for LineCode 13: 8\nRecord count for LineCode 2: 385\nNumber of columns for LineCode 2: 7\nRecord count for LineCode 25: 385\nNumber of columns for LineCode 25: 8\nRecord count for LineCode 3: 385\nNumber of columns for LineCode 3: 8\nRecord count for LineCode 34: 385\nNumber of columns for LineCode 34: 8\nRecord count for LineCode 35: 385\nNumber of columns for LineCode 35: 8\nRecord count for LineCode 36: 385\nNumber of columns for LineCode 36: 8\nRecord count for LineCode 45: 385\nNumber of columns for LineCode 45: 8\nRecord count for LineCode 50: 385\nNumber of columns for LineCode 50: 8\nRecord count for LineCode 51: 385\nNumber of columns for LineCode 51: 8\nRecord count for LineCode 56: 385\nNumber of columns for LineCode 56: 7\nRecord count for LineCode 59: 385\nNumber of columns for LineCode 59: 8\nRecord count for LineCode 6: 385\nNumber of columns for LineCode 6: 8\nRecord count for LineCode 60: 385\nNumber of columns for LineCode 60: 8\nRecord count for LineCode 64: 385\nNumber of columns for LineCode 64: 8\nRecord count for LineCode 65: 385\nNumber of columns for LineCode 65: 8\nRecord count for LineCode 68: 385\nNumber of columns for LineCode 68: 8\nRecord count for LineCode 69: 385\nNumber of columns for LineCode 69: 8\nRecord count for LineCode 70: 385\nNumber of columns for LineCode 70: 8\nRecord count for LineCode 75: 385\nNumber of columns for LineCode 75: 8\nRecord count for LineCode 76: 385\nNumber of columns for LineCode 76: 8\nRecord count for LineCode 79: 385\nNumber of columns for LineCode 79: 8\nRecord count for LineCode 82: 385\nNumber of columns for LineCode 82: 8\nRecord count for LineCode 83: 385\nNumber of columns for LineCode 83: 7\nRecord count for LineCode 87: 385\nNumber of columns for LineCode 87: 8\nRecord count for LineCode 88: 385\nNumber of columns for LineCode 88: 8\nRecord count for LineCode 89: 385\nNumber of columns for LineCode 89: 8\nRecord count for LineCode 90: 385\nNumber of columns for LineCode 90: 8\nRecord count for LineCode 91: 385\nNumber of columns for LineCode 91: 8\nRecord count for LineCode 92: 385\nNumber of columns for LineCode 92: 8\n"
     ]
    }
   ],
   "source": [
    "import requests as r\n",
    "import ast\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"BEA Data Processing\").getOrCreate()\n",
    "\n",
    "# API and dataset details\n",
    "URL = 'https://apps.bea.gov/api/data'\n",
    "api_key = '722F4B0C-A996-459A-9E2D-6FF6393FB1DC'\n",
    "table_name = 'CAGDP9'\n",
    "line_codes = ['1', '10', '11', '12', '13', '2', '25', '3', '34', '35', '36', '45', '50', '51', '56', '59', '6', '60', '64', '65', '68', '69', '70', '75', '76', '79', '82', '83', '87', '88', '89', '90', '91', '92']\n",
    "\n",
    "# Process each LineCode\n",
    "for code in line_codes:\n",
    "    f_url = f\"{URL}?&UserID={api_key}&method=GETDATA&datasetname=Regional&TableName={table_name}&GeoFips=MSA&Year=2019&LineCode={code}&ResultFormat=JSON\"\n",
    "    res = r.get(f_url).text\n",
    "    res_dict = ast.literal_eval(res)\n",
    "\n",
    "    if 'Data' in res_dict['BEAAPI']['Results']:\n",
    "        df = spark.createDataFrame(res_dict['BEAAPI']['Results']['Data'])\n",
    "        record_count = df.count()\n",
    "        print(f\"Record count for LineCode {code}: {record_count}\")\n",
    "        num_columns = len(df.columns)\n",
    "        print(f\"Number of columns for LineCode {code}: {num_columns}\")        \n",
    "    #else:\n",
    "        #print(f\"No data available for LineCode {code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e48912b8-a4e2-4374-8012-9f74b88a0a02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LineCode 1 - Row Count: 8085, Column Count: 7, Column Names: ['CL_UNIT', 'Code', 'DataValue', 'GeoFips', 'GeoName', 'TimePeriod', 'UNIT_MULT']\nLineCode 10 - Row Count: 8085, Column Count: 8, Column Names: ['CL_UNIT', 'Code', 'DataValue', 'GeoFips', 'GeoName', 'TimePeriod', 'UNIT_MULT', 'NoteRef']\nLineCode 11 - Row Count: 8085, Column Count: 8, Column Names: ['CL_UNIT', 'Code', 'DataValue', 'GeoFips', 'GeoName', 'TimePeriod', 'UNIT_MULT', 'NoteRef']\nLineCode 12 - Row Count: 8085, Column Count: 8, Column Names: ['CL_UNIT', 'Code', 'DataValue', 'GeoFips', 'GeoName', 'TimePeriod', 'UNIT_MULT', 'NoteRef']\nLineCode 13 - Row Count: 8085, Column Count: 8, Column Names: ['CL_UNIT', 'Code', 'DataValue', 'GeoFips', 'GeoName', 'TimePeriod', 'UNIT_MULT', 'NoteRef']\nLineCode 2 - Row Count: 8085, Column Count: 7, Column Names: ['CL_UNIT', 'Code', 'DataValue', 'GeoFips', 'GeoName', 'TimePeriod', 'UNIT_MULT']\nLineCode 25 - Row Count: 8085, Column Count: 8, Column Names: ['CL_UNIT', 'Code', 'DataValue', 'GeoFips', 'GeoName', 'TimePeriod', 'UNIT_MULT', 'NoteRef']\nLineCode 3 - Row Count: 8085, Column Count: 8, Column Names: ['CL_UNIT', 'Code', 'DataValue', 'GeoFips', 'GeoName', 'TimePeriod', 'UNIT_MULT', 'NoteRef']\nLineCode 34 - Row Count: 8085, Column Count: 8, Column Names: ['CL_UNIT', 'Code', 'DataValue', 'GeoFips', 'GeoName', 'TimePeriod', 'UNIT_MULT', 'NoteRef']\nLineCode 35 - Row Count: 8085, Column Count: 8, Column Names: ['CL_UNIT', 'Code', 'DataValue', 'GeoFips', 'GeoName', 'TimePeriod', 'UNIT_MULT', 'NoteRef']\nLineCode 36 - Row Count: 8085, Column Count: 8, Column Names: ['CL_UNIT', 'Code', 'DataValue', 'GeoFips', 'GeoName', 'TimePeriod', 'UNIT_MULT', 'NoteRef']\nLineCode 45 - Row Count: 8085, Column Count: 8, Column Names: ['CL_UNIT', 'Code', 'DataValue', 'GeoFips', 'GeoName', 'TimePeriod', 'UNIT_MULT', 'NoteRef']\nLineCode 50 - Row Count: 8085, Column Count: 8, Column Names: ['CL_UNIT', 'Code', 'DataValue', 'GeoFips', 'GeoName', 'TimePeriod', 'UNIT_MULT', 'NoteRef']\nLineCode 51 - Row Count: 8085, Column Count: 8, Column Names: ['CL_UNIT', 'Code', 'DataValue', 'GeoFips', 'GeoName', 'TimePeriod', 'UNIT_MULT', 'NoteRef']\nLineCode 56 - Row Count: 8085, Column Count: 8, Column Names: ['CL_UNIT', 'Code', 'DataValue', 'GeoFips', 'GeoName', 'TimePeriod', 'UNIT_MULT', 'NoteRef']\nLineCode 59 - Row Count: 8085, Column Count: 8, Column Names: ['CL_UNIT', 'Code', 'DataValue', 'GeoFips', 'GeoName', 'TimePeriod', 'UNIT_MULT', 'NoteRef']\nLineCode 6 - Row Count: 8085, Column Count: 8, Column Names: ['CL_UNIT', 'Code', 'DataValue', 'GeoFips', 'GeoName', 'TimePeriod', 'UNIT_MULT', 'NoteRef']\nLineCode 60 - Row Count: 8085, Column Count: 8, Column Names: ['CL_UNIT', 'Code', 'DataValue', 'GeoFips', 'GeoName', 'TimePeriod', 'UNIT_MULT', 'NoteRef']\nLineCode 64 - Row Count: 8085, Column Count: 8, Column Names: ['CL_UNIT', 'Code', 'DataValue', 'GeoFips', 'GeoName', 'TimePeriod', 'UNIT_MULT', 'NoteRef']\nLineCode 65 - Row Count: 8085, Column Count: 8, Column Names: ['CL_UNIT', 'Code', 'DataValue', 'GeoFips', 'GeoName', 'TimePeriod', 'UNIT_MULT', 'NoteRef']\nLineCode 68 - Row Count: 8085, Column Count: 8, Column Names: ['CL_UNIT', 'Code', 'DataValue', 'GeoFips', 'GeoName', 'TimePeriod', 'UNIT_MULT', 'NoteRef']\nLineCode 69 - Row Count: 8085, Column Count: 8, Column Names: ['CL_UNIT', 'Code', 'DataValue', 'GeoFips', 'GeoName', 'TimePeriod', 'UNIT_MULT', 'NoteRef']\nLineCode 70 - Row Count: 8085, Column Count: 8, Column Names: ['CL_UNIT', 'Code', 'DataValue', 'GeoFips', 'GeoName', 'TimePeriod', 'UNIT_MULT', 'NoteRef']\nLineCode 75 - Row Count: 8085, Column Count: 8, Column Names: ['CL_UNIT', 'Code', 'DataValue', 'GeoFips', 'GeoName', 'TimePeriod', 'UNIT_MULT', 'NoteRef']\nLineCode 76 - Row Count: 8085, Column Count: 8, Column Names: ['CL_UNIT', 'Code', 'DataValue', 'GeoFips', 'GeoName', 'TimePeriod', 'UNIT_MULT', 'NoteRef']\nLineCode 79 - Row Count: 8085, Column Count: 8, Column Names: ['CL_UNIT', 'Code', 'DataValue', 'GeoFips', 'GeoName', 'TimePeriod', 'UNIT_MULT', 'NoteRef']\nLineCode 82 - Row Count: 8085, Column Count: 8, Column Names: ['CL_UNIT', 'Code', 'DataValue', 'GeoFips', 'GeoName', 'TimePeriod', 'UNIT_MULT', 'NoteRef']\nLineCode 83 - Row Count: 8085, Column Count: 7, Column Names: ['CL_UNIT', 'Code', 'DataValue', 'GeoFips', 'GeoName', 'TimePeriod', 'UNIT_MULT']\nLineCode 87 - Row Count: 8085, Column Count: 8, Column Names: ['CL_UNIT', 'Code', 'DataValue', 'GeoFips', 'GeoName', 'TimePeriod', 'UNIT_MULT', 'NoteRef']\nLineCode 88 - Row Count: 8085, Column Count: 8, Column Names: ['CL_UNIT', 'Code', 'DataValue', 'GeoFips', 'GeoName', 'TimePeriod', 'UNIT_MULT', 'NoteRef']\nLineCode 89 - Row Count: 8085, Column Count: 8, Column Names: ['CL_UNIT', 'Code', 'DataValue', 'GeoFips', 'GeoName', 'TimePeriod', 'UNIT_MULT', 'NoteRef']\nLineCode 90 - Row Count: 8085, Column Count: 8, Column Names: ['CL_UNIT', 'Code', 'DataValue', 'GeoFips', 'GeoName', 'TimePeriod', 'UNIT_MULT', 'NoteRef']\nLineCode 91 - Row Count: 8085, Column Count: 8, Column Names: ['CL_UNIT', 'Code', 'DataValue', 'GeoFips', 'GeoName', 'NoteRef', 'TimePeriod', 'UNIT_MULT']\nLineCode 92 - Row Count: 8085, Column Count: 8, Column Names: ['CL_UNIT', 'Code', 'DataValue', 'GeoFips', 'GeoName', 'NoteRef', 'TimePeriod', 'UNIT_MULT']\n"
     ]
    }
   ],
   "source": [
    "import requests as r\n",
    "import ast\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"BEA Data Processing\").getOrCreate()\n",
    "\n",
    "# API and dataset details\n",
    "URL = 'https://apps.bea.gov/api/data'\n",
    "api_key = '722F4B0C-A996-459A-9E2D-6FF6393FB1DC'\n",
    "table_name = 'CAGDP2'\n",
    "\n",
    "# Function to retrieve LineCode values for a table\n",
    "def get_line_codes(api_key, table_name):\n",
    "    line_code_url = f\"{URL}?UserID={api_key}&method=GetParameterValuesFiltered&datasetname=Regional&TargetParameter=LineCode&TableName={table_name}&ResultFormat=json\"\n",
    "    response = r.get(line_code_url)\n",
    "    if response.status_code == 200:\n",
    "        res_dict = response.json()\n",
    "        return [item['Key'] for item in res_dict['BEAAPI']['Results']['ParamValue']]\n",
    "    else:\n",
    "        print(f\"Failed to retrieve LineCode values for Table {table_name}. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "# Get LineCode values for the table\n",
    "line_codes = get_line_codes(api_key, table_name)\n",
    "\n",
    "# Process each LineCode\n",
    "for code in line_codes:\n",
    "    data_url = f\"{URL}?&UserID={api_key}&method=GETDATA&datasetname=Regional&TableName={table_name}&GeoFips=MSA&Year=ALL&LineCode={code}&ResultFormat=JSON\"\n",
    "    res = r.get(data_url).text\n",
    "    res_dict = ast.literal_eval(res)\n",
    "\n",
    "    if 'Data' in res_dict['BEAAPI']['Results']:\n",
    "        df = spark.createDataFrame(res_dict['BEAAPI']['Results']['Data'])\n",
    "        row_count = df.count()\n",
    "        col_count = len(df.columns)\n",
    "        column_names = df.columns\n",
    "        print(f\"LineCode {code} - Row Count: {row_count}, Column Count: {col_count}, Column Names: {column_names}\")\n",
    "    else:\n",
    "        print(f\"No data available for LineCode {code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c0340a4-a9dd-4f6a-aa80-a3099976d1b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve LineCode values for Table CAGDP2. Status code: 429\n"
     ]
    }
   ],
   "source": [
    "import requests as r\n",
    "import ast\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"BEA Data Processing\").getOrCreate()\n",
    "\n",
    "# API and dataset details\n",
    "URL = 'https://apps.bea.gov/api/data'\n",
    "api_key = '722F4B0C-A996-459A-9E2D-6FF6393FB1DC'\n",
    "table_name = 'CAGDP2'\n",
    "\n",
    "# Function to retrieve LineCode values for a table\n",
    "def get_line_codes(api_key, table_name):\n",
    "    line_code_url = f\"{URL}?UserID={api_key}&method=GetParameterValuesFiltered&datasetname=Regional&TargetParameter=LineCode&TableName={table_name}&ResultFormat=json\"\n",
    "    response = r.get(line_code_url)\n",
    "    if response.status_code == 200:\n",
    "        res_dict = response.json()\n",
    "        return [item['Key'] for item in res_dict['BEAAPI']['Results']['ParamValue']]\n",
    "    else:\n",
    "        print(f\"Failed to retrieve LineCode values for Table {table_name}. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "# Get LineCode values for the table\n",
    "line_codes = get_line_codes(api_key, table_name)\n",
    "\n",
    "# Process each LineCode\n",
    "for code in line_codes:\n",
    "    data_url = f\"{URL}?&UserID={api_key}&method=GETDATA&datasetname=Regional&TableName={table_name}&GeoFips=MSA&Year=ALL&LineCode={code}&ResultFormat=JSON\"\n",
    "    res = r.get(data_url).text\n",
    "    res_dict = ast.literal_eval(res)\n",
    "\n",
    "    if 'Data' in res_dict['BEAAPI']['Results']:\n",
    "        df = spark.createDataFrame(res_dict['BEAAPI']['Results']['Data'])\n",
    "\n",
    "        # Add 'NoteRef' column if it's missing\n",
    "        if 'NoteRef' not in df.columns:\n",
    "            df = df.withColumn('NoteRef', lit(None))\n",
    "\n",
    "        row_count = df.count()\n",
    "        col_count = len(df.columns)\n",
    "        column_names = df.columns\n",
    "        print(f\"LineCode {code} - Row Count: {row_count}, Column Count: {col_count}, Column Names: {column_names}\")\n",
    "    else:\n",
    "        print(f\"No data available for LineCode {code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53fe004c-4311-4403-b95a-9157ab583242",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve LineCode values for Table CAGDP2. Status code: 429\nNo data available for any LineCode\n"
     ]
    }
   ],
   "source": [
    "import requests as r\n",
    "import ast\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"BEA Data Processing\").getOrCreate()\n",
    "\n",
    "# API and dataset details\n",
    "URL = 'https://apps.bea.gov/api/data'\n",
    "api_key = '722F4B0C-A996-459A-9E2D-6FF6393FB1DC'\n",
    "table_name = 'CAGDP2'\n",
    "\n",
    "# Standard column order\n",
    "standard_columns = ['CL_UNIT', 'Code', 'DataValue', 'GeoFips', 'GeoName', 'TimePeriod', 'UNIT_MULT', 'NoteRef']\n",
    "\n",
    "# Function to retrieve LineCode values for a table\n",
    "def get_line_codes(api_key, table_name):\n",
    "    line_code_url = f\"{URL}?UserID={api_key}&method=GetParameterValuesFiltered&datasetname=Regional&TargetParameter=LineCode&TableName={table_name}&ResultFormat=json\"\n",
    "    response = r.get(line_code_url)\n",
    "    if response.status_code == 200:\n",
    "        res_dict = response.json()\n",
    "        return [item['Key'] for item in res_dict['BEAAPI']['Results']['ParamValue']]\n",
    "    else:\n",
    "        print(f\"Failed to retrieve LineCode values for Table {table_name}. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "# Get LineCode values for the table\n",
    "line_codes = get_line_codes(api_key, table_name)\n",
    "\n",
    "# Initialize an empty DataFrame for union\n",
    "union_df = None\n",
    "\n",
    "# Process each LineCode\n",
    "for code in line_codes:\n",
    "    data_url = f\"{URL}?&UserID={api_key}&method=GETDATA&datasetname=Regional&TableName={table_name}&GeoFips=MSA&Year=ALL&LineCode={code}&ResultFormat=JSON\"\n",
    "    res = r.get(data_url).text\n",
    "    res_dict = ast.literal_eval(res)\n",
    "\n",
    "    if 'Data' in res_dict['BEAAPI']['Results']:\n",
    "        df = spark.createDataFrame(res_dict['BEAAPI']['Results']['Data'])\n",
    "\n",
    "        # Add 'NoteRef' column if it's missing\n",
    "        if 'NoteRef' not in df.columns:\n",
    "            df = df.withColumn('NoteRef', lit(None))\n",
    "\n",
    "        # Reorder columns to match the standard order\n",
    "        df = df.select(*standard_columns)\n",
    "\n",
    "        # Union the DataFrame with the union_df\n",
    "        if union_df is None:\n",
    "            union_df = df\n",
    "        else:\n",
    "            union_df = union_df.unionByName(df)\n",
    "\n",
    "# Create a temporary view from the unified DataFrame\n",
    "if union_df is not None:\n",
    "    union_df.createOrReplaceTempView(\"union_view\")\n",
    "\n",
    "    # Execute an SQL query to display the data\n",
    "    result_df = spark.sql(\"SELECT * FROM union_view\")\n",
    "    result_df.show()  # Displaying data as a table\n",
    "else:\n",
    "    print(\"No data available for any LineCode\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fda4c3b-07f5-4fa2-86dc-b10be9764a61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4443003582982629>, line 31\u001B[0m\n",
       "\u001B[1;32m     28\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m []\n",
       "\u001B[1;32m     30\u001B[0m \u001B[38;5;66;03m# Get LineCode values for the table\u001B[39;00m\n",
       "\u001B[0;32m---> 31\u001B[0m line_codes \u001B[38;5;241m=\u001B[39m get_line_codes(API_KEY, TABLE_NAME)\n",
       "\u001B[1;32m     33\u001B[0m \u001B[38;5;66;03m# Initialize an empty DataFrame for union\u001B[39;00m\n",
       "\u001B[1;32m     34\u001B[0m union_df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m<command-4443003582982629>, line 25\u001B[0m, in \u001B[0;36mget_line_codes\u001B[0;34m(api_key, table_name)\u001B[0m\n",
       "\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m200\u001B[39m:\n",
       "\u001B[1;32m     24\u001B[0m     res_dict \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mjson()\n",
       "\u001B[0;32m---> 25\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [item[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mKey\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m \u001B[43mres_dict\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mBEAAPI\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mResults\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mParamValue\u001B[39m\u001B[38;5;124m'\u001B[39m]]\n",
       "\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m     27\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to retrieve LineCode values for Table \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtable_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Status code: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;241m.\u001B[39mstatus_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "\u001B[0;31mKeyError\u001B[0m: 'Results'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)\nFile \u001B[0;32m<command-4443003582982629>, line 31\u001B[0m\n\u001B[1;32m     28\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m []\n\u001B[1;32m     30\u001B[0m \u001B[38;5;66;03m# Get LineCode values for the table\u001B[39;00m\n\u001B[0;32m---> 31\u001B[0m line_codes \u001B[38;5;241m=\u001B[39m get_line_codes(API_KEY, TABLE_NAME)\n\u001B[1;32m     33\u001B[0m \u001B[38;5;66;03m# Initialize an empty DataFrame for union\u001B[39;00m\n\u001B[1;32m     34\u001B[0m union_df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\nFile \u001B[0;32m<command-4443003582982629>, line 25\u001B[0m, in \u001B[0;36mget_line_codes\u001B[0;34m(api_key, table_name)\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m200\u001B[39m:\n\u001B[1;32m     24\u001B[0m     res_dict \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mjson()\n\u001B[0;32m---> 25\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [item[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mKey\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m \u001B[43mres_dict\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mBEAAPI\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mResults\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mParamValue\u001B[39m\u001B[38;5;124m'\u001B[39m]]\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     27\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to retrieve LineCode values for Table \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtable_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Status code: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;241m.\u001B[39mstatus_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\n\u001B[0;31mKeyError\u001B[0m: 'Results'",
       "errorSummary": "<span class='ansi-red-fg'>KeyError</span>: 'Results'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests as r\n",
    "import ast\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Parameters (These can be modified as needed)\n",
    "URL = 'https://apps.bea.gov/api/data'\n",
    "API_KEY = '722F4B0C-A996-459A-9E2D-6FF6393FB1DC'\n",
    "TABLE_NAME = 'CAGDP2'         \n",
    "GEOFIPS = 'MSA'                       \n",
    "YEAR = 'ALL'                          \n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"BEA Data Processing\").getOrCreate()\n",
    "\n",
    "# Standard column order\n",
    "STANDARD_COLUMNS = ['CL_UNIT', 'Code', 'DataValue', 'GeoFips', 'GeoName', 'TimePeriod', 'UNIT_MULT', 'NoteRef']\n",
    "\n",
    "# Function to retrieve LineCode values for a table\n",
    "def get_line_codes(api_key, table_name):\n",
    "    line_code_url = f\"{URL}?UserID={api_key}&method=GetParameterValuesFiltered&datasetname=Regional&TargetParameter=LineCode&TableName={table_name}&ResultFormat=json\"\n",
    "    response = r.get(line_code_url)\n",
    "    if response.status_code == 200:\n",
    "        res_dict = response.json()\n",
    "        return [item['Key'] for item in res_dict['BEAAPI']['Results']['ParamValue']]\n",
    "    else:\n",
    "        print(f\"Failed to retrieve LineCode values for Table {table_name}. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "# Get LineCode values for the table\n",
    "line_codes = get_line_codes(API_KEY, TABLE_NAME)\n",
    "\n",
    "# Initialize an empty DataFrame for union\n",
    "union_df = None\n",
    "\n",
    "# Process each LineCode\n",
    "for code in line_codes:\n",
    "    data_url = f\"{URL}?&UserID={API_KEY}&method=GETDATA&datasetname=Regional&TableName={TABLE_NAME}&GeoFips={GEOFIPS}&Year={YEAR}&LineCode={code}&ResultFormat=JSON\"\n",
    "    res = r.get(data_url).text\n",
    "    res_dict = ast.literal_eval(res)\n",
    "\n",
    "    if 'Data' in res_dict['BEAAPI']['Results']:\n",
    "        df = spark.createDataFrame(res_dict['BEAAPI']['Results']['Data'])\n",
    "\n",
    "        # Add missing columns from STANDARD_COLUMNS\n",
    "        for col in STANDARD_COLUMNS:\n",
    "            if col not in df.columns:\n",
    "                df = df.withColumn(col, lit(None))\n",
    "\n",
    "        # Reorder DataFrame columns to match STANDARD_COLUMNS\n",
    "        df = df.select(*STANDARD_COLUMNS)\n",
    "\n",
    "        # Union the DataFrame with union_df\n",
    "        if union_df is None:\n",
    "            union_df = df\n",
    "        else:\n",
    "            union_df = union_df.unionByName(df)\n",
    "\n",
    "# After processing all LineCodes\n",
    "if union_df is not None:\n",
    "    union_df.createOrReplaceTempView(\"union_view\")\n",
    "    result_df = spark.sql(\"SELECT * FROM union_view\")\n",
    "    result_df.show()  \n",
    "else:\n",
    "    print(\"No data available for any LineCode\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad471fbe-e8ff-4f5c-9d58-e6e9d0c96f45",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests as r\n",
    "import ast\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "def get_column_names(api_key, tables, URL):\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder.appName(\"BEA Data Column Names\").getOrCreate()\n",
    "\n",
    "    # Function to get a sample of data to extract column names\n",
    "    def get_columns(table_name, line_code):\n",
    "        sample_url = f\"{URL}?UserID={api_key}&method=GETDATA&datasetname=Regional&TableName={table_name}&GeoFips=MSA&Year=2010&LineCode={line_code}&ResultFormat=JSON\"\n",
    "        response = r.get(sample_url).json()\n",
    "        if 'Data' in response['BEAAPI']['Results']:\n",
    "            df = spark.createDataFrame(response['BEAAPI']['Results']['Data'])\n",
    "            return df.columns\n",
    "        return []\n",
    "\n",
    "    # Collecting column names from all tables\n",
    "    unique_columns = set()\n",
    "    for table_name in tables:\n",
    "        res = r.get(f\"{URL}/?UserID={api_key}&method=GetParameterValuesFiltered&datasetname=Regional&TargetParameter=LineCode&TableName={table_name}&ResultFormat=json\").json()\n",
    "        line_codes = [item['Key'] for item in res['BEAAPI']['Results']['ParamValue']]\n",
    "\n",
    "        for line_code in line_codes:\n",
    "            columns = get_columns(table_name, line_code)\n",
    "            unique_columns.update(columns)\n",
    "\n",
    "    return list(unique_columns)\n",
    "\n",
    "def full_load(api_key, dataset_name, table_name):\n",
    "    url = 'https://apps.bea.gov/api/data'\n",
    "    line_codes = []\n",
    "    union_df = None\n",
    "\n",
    "    tables = [table_name]  # List of tables\n",
    "    STANDARD_COLUMNS = get_column_names(api_key, tables, url)  # Dynamically get column names\n",
    "\n",
    "    # Fetch Line Codes\n",
    "    res = r.get(url + f'''?UserID={api_key}&method=GetParameterValuesFiltered&datasetname={dataset_name}&TargetParameter=LineCode&TableName={table_name}&ResultFormat=json''').json()\n",
    "\n",
    "    if 'Error' in res['BEAAPI'].keys():\n",
    "        if res['BEAAPI']['Error']['APIErrorCode'] == 429:\n",
    "            time.sleep(60)\n",
    "        else:\n",
    "            print(\"Error Code: \" + res['BEAAPI']['Error']['APIErrorCode'] + \"\\nError Description: \" + res['BEAAPI']['Error']['ErrorDetail']['Description'])\n",
    "    else:\n",
    "        for codeno in res['BEAAPI']['Results']['ParamValue']:\n",
    "            line_codes.append(codeno['Key'])\n",
    "\n",
    "    for code in line_codes:\n",
    "        f_url = url + '?&UserID=' + f'{api_key}&method=GETDATA&datasetname={dataset_name}&TableName={table_name}&GeoFips=MSA&Year=ALL&LineCode={code}&ResultFormat=JSON'\n",
    "        res = r.get(f_url).text\n",
    "        res = ast.literal_eval(res)\n",
    "        if 'Error' in res['BEAAPI'].keys():\n",
    "            if res['BEAAPI']['Error']['APIErrorCode'] == 429:\n",
    "                time.sleep(60)\n",
    "            else:\n",
    "                print(\"Error Code: \" + res['BEAAPI']['Error']['APIErrorCode'] + \"\\nError Description: \" + res['BEAAPI']['Error']['ErrorDetail']['Description'])\n",
    "        else:\n",
    "            if 'Data' in res['BEAAPI']['Results']:\n",
    "                df = spark.createDataFrame(res['BEAAPI']['Results']['Data'])\n",
    "\n",
    "                # Add missing columns from STANDARD_COLUMNS\n",
    "                for col in STANDARD_COLUMNS:\n",
    "                    if col not in df.columns:\n",
    "                        df = df.withColumn(col, lit(None).cast('string'))\n",
    "\n",
    "                # Reorder DataFrame columns to match STANDARD_COLUMNS\n",
    "                df = df.select(*STANDARD_COLUMNS)\n",
    "\n",
    "                # Union the DataFrame with union_df\n",
    "                if union_df is None:\n",
    "                    union_df = df\n",
    "                else:\n",
    "                    union_df = union_df.unionByName(df)\n",
    "\n",
    "    if union_df is not None:\n",
    "        union_df.createOrReplaceTempView(\"union_view\")\n",
    "        spark.sql(f'''create or replace table {table_name} AS SELECT * FROM union_view ''')\n",
    "        print(f'''{table_name} refreshed for all years ''')\n",
    "\n",
    "full_load('F4993EB8-9C0D-4141-9E37-828C00D09143','Regional','CAGDP1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "126cf237-1a3e-41eb-8bc9-e5cbed8b80c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3093885186637651>, line 97\u001B[0m\n",
       "\u001B[1;32m     93\u001B[0m         result_df\u001B[38;5;241m.\u001B[39mshow()  \u001B[38;5;66;03m# Displaying data as a table; adjust the number of rows as needed\u001B[39;00m\n",
       "\u001B[1;32m     96\u001B[0m \u001B[38;5;66;03m# Example usage\u001B[39;00m\n",
       "\u001B[0;32m---> 97\u001B[0m full_load(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mF4993EB8-9C0D-4141-9E37-828C00D09143\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRegional\u001B[39m\u001B[38;5;124m'\u001B[39m, [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCAGDP1\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCAGDP2\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
       "\n",
       "File \u001B[0;32m<command-3093885186637651>, line 37\u001B[0m, in \u001B[0;36mfull_load\u001B[0;34m(api_key, dataset_name, tables)\u001B[0m\n",
       "\u001B[1;32m     34\u001B[0m union_df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# Get unique column names across all tables\u001B[39;00m\n",
       "\u001B[0;32m---> 37\u001B[0m STANDARD_COLUMNS \u001B[38;5;241m=\u001B[39m \u001B[43mget_column_names\u001B[49m\u001B[43m(\u001B[49m\u001B[43mapi_key\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtables\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m table_name \u001B[38;5;129;01min\u001B[39;00m tables:\n",
       "\u001B[1;32m     40\u001B[0m     line_codes \u001B[38;5;241m=\u001B[39m []\n",
       "\n",
       "File \u001B[0;32m<command-3093885186637651>, line 24\u001B[0m, in \u001B[0;36mget_column_names\u001B[0;34m(api_key, tables, url)\u001B[0m\n",
       "\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m table_name \u001B[38;5;129;01min\u001B[39;00m tables:\n",
       "\u001B[1;32m     23\u001B[0m     res \u001B[38;5;241m=\u001B[39m r\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00murl\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/?UserID=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mapi_key\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m&method=GetParameterValuesFiltered&datasetname=Regional&TargetParameter=LineCode&TableName=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtable_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m&ResultFormat=json\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mjson()\n",
       "\u001B[0;32m---> 24\u001B[0m     line_codes \u001B[38;5;241m=\u001B[39m [item[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mKey\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m \u001B[43mres\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mBEAAPI\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mResults\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mParamValue\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m]\n",
       "\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m line_code \u001B[38;5;129;01min\u001B[39;00m line_codes:\n",
       "\u001B[1;32m     27\u001B[0m         columns \u001B[38;5;241m=\u001B[39m get_columns(table_name, line_code)\n",
       "\n",
       "\u001B[0;31mKeyError\u001B[0m: 'ParamValue'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)\nFile \u001B[0;32m<command-3093885186637651>, line 97\u001B[0m\n\u001B[1;32m     93\u001B[0m         result_df\u001B[38;5;241m.\u001B[39mshow()  \u001B[38;5;66;03m# Displaying data as a table; adjust the number of rows as needed\u001B[39;00m\n\u001B[1;32m     96\u001B[0m \u001B[38;5;66;03m# Example usage\u001B[39;00m\n\u001B[0;32m---> 97\u001B[0m full_load(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mF4993EB8-9C0D-4141-9E37-828C00D09143\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRegional\u001B[39m\u001B[38;5;124m'\u001B[39m, [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCAGDP1\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCAGDP2\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\nFile \u001B[0;32m<command-3093885186637651>, line 37\u001B[0m, in \u001B[0;36mfull_load\u001B[0;34m(api_key, dataset_name, tables)\u001B[0m\n\u001B[1;32m     34\u001B[0m union_df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# Get unique column names across all tables\u001B[39;00m\n\u001B[0;32m---> 37\u001B[0m STANDARD_COLUMNS \u001B[38;5;241m=\u001B[39m \u001B[43mget_column_names\u001B[49m\u001B[43m(\u001B[49m\u001B[43mapi_key\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtables\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m table_name \u001B[38;5;129;01min\u001B[39;00m tables:\n\u001B[1;32m     40\u001B[0m     line_codes \u001B[38;5;241m=\u001B[39m []\n\nFile \u001B[0;32m<command-3093885186637651>, line 24\u001B[0m, in \u001B[0;36mget_column_names\u001B[0;34m(api_key, tables, url)\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m table_name \u001B[38;5;129;01min\u001B[39;00m tables:\n\u001B[1;32m     23\u001B[0m     res \u001B[38;5;241m=\u001B[39m r\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00murl\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/?UserID=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mapi_key\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m&method=GetParameterValuesFiltered&datasetname=Regional&TargetParameter=LineCode&TableName=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtable_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m&ResultFormat=json\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mjson()\n\u001B[0;32m---> 24\u001B[0m     line_codes \u001B[38;5;241m=\u001B[39m [item[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mKey\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m \u001B[43mres\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mBEAAPI\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mResults\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mParamValue\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m]\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m line_code \u001B[38;5;129;01min\u001B[39;00m line_codes:\n\u001B[1;32m     27\u001B[0m         columns \u001B[38;5;241m=\u001B[39m get_columns(table_name, line_code)\n\n\u001B[0;31mKeyError\u001B[0m: 'ParamValue'",
       "errorSummary": "<span class='ansi-red-fg'>KeyError</span>: 'ParamValue'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests as r\n",
    "import ast\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "def get_column_names(api_key, tables, url):\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder.appName(\"BEA Data Column Names\").getOrCreate()\n",
    "\n",
    "    # Function to get a sample of data to extract column names\n",
    "    def get_columns(table_name, line_code):\n",
    "        sample_url = f\"{url}?UserID={api_key}&method=GETDATA&datasetname=Regional&TableName={table_name}&GeoFips=MSA&Year=2010&LineCode={line_code}&ResultFormat=JSON\"\n",
    "        response = r.get(sample_url).json()\n",
    "        if 'Data' in response['BEAAPI']['Results']:\n",
    "            df = spark.createDataFrame(response['BEAAPI']['Results']['Data'])\n",
    "            return df.columns\n",
    "        return []\n",
    "\n",
    "    # Collecting column names from all tables\n",
    "    unique_columns = set()\n",
    "    for table_name in tables:\n",
    "        res = r.get(f\"{url}/?UserID={api_key}&method=GetParameterValuesFiltered&datasetname=Regional&TargetParameter=LineCode&TableName={table_name}&ResultFormat=json\").json()\n",
    "        line_codes = [item['Key'] for item in res['BEAAPI']['Results']['ParamValue']]\n",
    "\n",
    "        for line_code in line_codes:\n",
    "            columns = get_columns(table_name, line_code)\n",
    "            unique_columns.update(columns)\n",
    "\n",
    "    return list(unique_columns)\n",
    "\n",
    "def full_load(api_key, dataset_name, tables):\n",
    "    url = 'https://apps.bea.gov/api/data'\n",
    "    union_df = None\n",
    "\n",
    "    # Get unique column names across all tables\n",
    "    STANDARD_COLUMNS = get_column_names(api_key, tables, url)\n",
    "\n",
    "    for table_name in tables:\n",
    "        line_codes = []\n",
    "        # Fetch Line Codes for the current table\n",
    "        res = r.get(url + f\"?UserID={api_key}&method=GetParameterValuesFiltered&datasetname={dataset_name}&TargetParameter=LineCode&TableName={table_name}&ResultFormat=json\").json()\n",
    "\n",
    "        if 'Error' in res['BEAAPI'].keys():\n",
    "            if res['BEAAPI']['Error']['APIErrorCode'] == 429:\n",
    "                time.sleep(60)\n",
    "            else:\n",
    "                print(\"Error Code: \" + res['BEAAPI']['Error']['APIErrorCode'] + \"\\nError Description: \" + res['BEAAPI']['Error']['ErrorDetail']['Description'])\n",
    "            continue\n",
    "\n",
    "        for codeno in res['BEAAPI']['Results']['ParamValue']:\n",
    "            line_codes.append(codeno['Key'])\n",
    "\n",
    "        # Fetch and process data for each LineCode\n",
    "        for code in line_codes:\n",
    "            f_url = url + f\"?&UserID={api_key}&method=GETDATA&datasetname={dataset_name}&TableName={table_name}&GeoFips=MSA&Year=ALL&LineCode={code}&ResultFormat=JSON\"\n",
    "            res = r.get(f_url).text\n",
    "            res_dict = ast.literal_eval(res)\n",
    "\n",
    "            if 'Error' in res_dict['BEAAPI'].keys():\n",
    "                if res_dict['BEAAPI']['Error']['APIErrorCode'] == 429:\n",
    "                    time.sleep(60)\n",
    "                else:\n",
    "                    print(\"Error Code: \" + res_dict['BEAAPI']['Error']['APIErrorCode'] + \"\\nError Description: \" + res_dict['BEAAPI']['Error']['ErrorDetail']['Description'])\n",
    "                continue\n",
    "\n",
    "            if 'Data' in res_dict['BEAAPI']['Results']:\n",
    "                df = spark.createDataFrame(res_dict['BEAAPI']['Results']['Data'])\n",
    "\n",
    "                # Add missing columns from STANDARD_COLUMNS\n",
    "                for col in STANDARD_COLUMNS:\n",
    "                    if col not in df.columns:\n",
    "                        df = df.withColumn(col, lit(None).cast('string'))\n",
    "\n",
    "                # Reorder DataFrame columns to match STANDARD_COLUMNS\n",
    "                df = df.select(*STANDARD_COLUMNS)\n",
    "\n",
    "                # Union the DataFrame with union_df\n",
    "                if union_df is None:\n",
    "                    union_df = df\n",
    "                else:\n",
    "                    union_df = union_df.unionByName(df)\n",
    "\n",
    "        if union_df is not None:\n",
    "            #union_df.createOrReplaceTempView(\"union_view\")\n",
    "            #union_df.show()\n",
    "            #union_df.printSchema()\n",
    "            #spark.sql(f\"create or replace table {table_name} AS SELECT * FROM union_view\")\n",
    "            #print(f\"{table_name} refreshed for all years\")\n",
    "   \n",
    "            union_df.createOrReplaceTempView(\"union_view\")\n",
    "        result_df = spark.sql(\"SELECT DISTINCT Code FROM union_view\")\n",
    "        result_df.show()  # Displaying data as a table; adjust the number of rows as needed\n",
    "\n",
    "\n",
    "# Example usage\n",
    "full_load('F4993EB8-9C0D-4141-9E37-828C00D09143', 'Regional', ['CAGDP1', 'CAGDP2'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dd96782-759f-4344-9773-8e180472069c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Full Load Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1766b3d-4d23-4d3b-bcd8-1e1a9af7de5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n|     Code|\n+---------+\n| CAGDP1-1|\n| CAGDP1-2|\n| CAGDP1-3|\n| CAGDP2-1|\n|CAGDP2-10|\n|CAGDP2-11|\n|CAGDP2-12|\n|CAGDP2-13|\n| CAGDP2-2|\n|CAGDP2-25|\n| CAGDP2-3|\n|CAGDP2-34|\n|CAGDP2-35|\n|CAGDP2-36|\n|CAGDP2-45|\n|CAGDP2-50|\n|CAGDP2-51|\n|CAGDP2-56|\n|CAGDP2-59|\n| CAGDP2-6|\n+---------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "import requests as r\n",
    "import ast\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "def get_column_names(api_key, tables, url):\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder.appName(\"BEA Data Column Names\").getOrCreate()\n",
    "\n",
    "    # Function to get a sample of data to extract column names\n",
    "    def get_columns(table_name, line_code):\n",
    "        sample_url = f\"{url}?UserID={api_key}&method=GETDATA&datasetname=Regional&TableName={table_name}&GeoFips=MSA&Year=ALL&LineCode={line_code}&ResultFormat=JSON\"\n",
    "        response = r.get(sample_url).json()\n",
    "        if 'Data' in response['BEAAPI']['Results']:\n",
    "            df = spark.createDataFrame(response['BEAAPI']['Results']['Data'])\n",
    "            return df.columns\n",
    "        return []\n",
    "\n",
    "    # Collecting column names from all tables\n",
    "    unique_columns = set()\n",
    "    for table_name in tables:\n",
    "        res = r.get(f\"{url}/?UserID={api_key}&method=GetParameterValuesFiltered&datasetname=Regional&TargetParameter=LineCode&TableName={table_name}&ResultFormat=json\").json()\n",
    "        line_codes = [item['Key'] for item in res['BEAAPI']['Results']['ParamValue']]\n",
    "\n",
    "        for line_code in line_codes:\n",
    "            columns = get_columns(table_name, line_code)\n",
    "            unique_columns.update(columns)\n",
    "\n",
    "    return list(unique_columns)\n",
    "\n",
    "def full_load(api_key, dataset_name, tables):\n",
    "    url = 'https://apps.bea.gov/api/data'\n",
    "    union_df = None\n",
    "\n",
    "    # Get unique column names across all tables\n",
    "    STANDARD_COLUMNS = get_column_names(api_key, tables, url)\n",
    "\n",
    "    for table_name in tables:\n",
    "        line_codes = []\n",
    "        # Fetch Line Codes for the current table\n",
    "        res = r.get(url + f\"?UserID={api_key}&method=GetParameterValuesFiltered&datasetname={dataset_name}&TargetParameter=LineCode&TableName={table_name}&ResultFormat=json\").json()\n",
    "\n",
    "        if 'Error' in res['BEAAPI'].keys():\n",
    "            if res['BEAAPI']['Error']['APIErrorCode'] == 429:\n",
    "                time.sleep(60)\n",
    "            else:\n",
    "                print(\"Error Code: \" + res['BEAAPI']['Error']['APIErrorCode'] + \"\\nError Description: \" + res['BEAAPI']['Error']['ErrorDetail']['Description'])\n",
    "            continue\n",
    "\n",
    "        for codeno in res['BEAAPI']['Results']['ParamValue']:\n",
    "            line_codes.append(codeno['Key'])\n",
    "\n",
    "        # Fetch and process data for each LineCode\n",
    "        for code in line_codes:\n",
    "            f_url = url + f\"?&UserID={api_key}&method=GETDATA&datasetname={dataset_name}&TableName={table_name}&GeoFips=MSA&Year=ALL&LineCode={code}&ResultFormat=JSON\"\n",
    "            res = r.get(f_url).text\n",
    "            res_dict = ast.literal_eval(res)\n",
    "\n",
    "            if 'Error' in res_dict['BEAAPI'].keys():\n",
    "                if res_dict['BEAAPI']['Error']['APIErrorCode'] == 429:\n",
    "                    time.sleep(60)\n",
    "                else:\n",
    "                    print(\"Error Code: \" + res_dict['BEAAPI']['Error']['APIErrorCode'] + \"\\nError Description: \" + res_dict['BEAAPI']['Error']['ErrorDetail']['Description'])\n",
    "                continue\n",
    "\n",
    "            if 'Data' in res_dict['BEAAPI']['Results']:\n",
    "                df = spark.createDataFrame(res_dict['BEAAPI']['Results']['Data'])\n",
    "\n",
    "                # Add missing columns from STANDARD_COLUMNS\n",
    "                for col in STANDARD_COLUMNS:\n",
    "                    if col not in df.columns:\n",
    "                        df = df.withColumn(col, lit(None).cast('string'))\n",
    "\n",
    "                # Reorder DataFrame columns to match STANDARD_COLUMNS\n",
    "                df = df.select(*STANDARD_COLUMNS)\n",
    "\n",
    "                # Union the DataFrame with union_df\n",
    "                if union_df is None:\n",
    "                    union_df = df\n",
    "                else:\n",
    "                    union_df = union_df.unionByName(df)\n",
    "\n",
    "    if union_df is not None:\n",
    "        union_df.createOrReplaceTempView(\"union_view\")\n",
    "        result_df = spark.sql(\"SELECT DISTINCT Code FROM union_view\")\n",
    "        result_df.show()  \n",
    "        #spark.sql(f\"create or replace table {table_name} AS SELECT * FROM union_view\")\n",
    "        #print(f\"{table_name} refreshed for all years\")\n",
    "\n",
    "# Example usage\n",
    "full_load('930BA9FE-78E2-40FB-A62E-C89ED7DA772F', 'Regional', ['CAGDP1', 'CAGDP2', 'CAGDP9'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7694cde5-e40f-402f-bbbc-96266d009410",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Incremental Load Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca2f933e-14dd-4956-8543-051e1ad70942",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n|    Code|TimePeriod|\n+--------+----------+\n|CAGDP1-1|      2010|\n|CAGDP1-2|      2010|\n|CAGDP1-3|      2010|\n+--------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "import requests as r\n",
    "import ast\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "def get_column_names(api_key, tables, URL):\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder.appName(\"BEA Data Column Names\").getOrCreate()\n",
    "\n",
    "    # Function to get a sample of data to extract column names\n",
    "    def get_columns(table_name, line_code):\n",
    "        sample_url = f\"{URL}?UserID={api_key}&method=GETDATA&datasetname=Regional&TableName={table_name}&GeoFips=MSA&Year=2010&LineCode={line_code}&ResultFormat=JSON\"\n",
    "        response = r.get(sample_url).json()\n",
    "        if 'Data' in response['BEAAPI']['Results']:\n",
    "            df = spark.createDataFrame(response['BEAAPI']['Results']['Data'])\n",
    "            return df.columns\n",
    "        return []\n",
    "\n",
    "    # Collecting column names from all tables\n",
    "    unique_columns = set()\n",
    "    for table_name in tables:\n",
    "        res = r.get(f\"{URL}/?UserID={api_key}&method=GetParameterValuesFiltered&datasetname=Regional&TargetParameter=LineCode&TableName={table_name}&ResultFormat=json\").json()\n",
    "        line_codes = [item['Key'] for item in res['BEAAPI']['Results']['ParamValue']]\n",
    "\n",
    "        for line_code in line_codes:\n",
    "            columns = get_columns(table_name, line_code)\n",
    "            unique_columns.update(columns)\n",
    "\n",
    "    return list(unique_columns)\n",
    "\n",
    "def incremental_load(api_key, dataset_name, table_name, v_year=None):\n",
    "    url = 'https://apps.bea.gov/api/data'\n",
    "    union_df = None\n",
    "\n",
    "    # Get unique column names for the table\n",
    "    STANDARD_COLUMNS = get_column_names(api_key, [table_name], url)\n",
    "\n",
    "    line_codes = []\n",
    "    # Fetch Line Codes for the current table\n",
    "    res = r.get(url + f\"?UserID={api_key}&method=GetParameterValuesFiltered&datasetname={dataset_name}&TargetParameter=LineCode&TableName={table_name}&ResultFormat=json\").json()\n",
    "\n",
    "    if 'Error' in res['BEAAPI'].keys():\n",
    "        if res['BEAAPI']['Error']['APIErrorCode'] == 429:\n",
    "            time.sleep(60)\n",
    "        else:\n",
    "            print(\"Error Code: \" + res['BEAAPI']['Error']['APIErrorCode'] + \"\\nError Description: \" + res['BEAAPI']['Error']['ErrorDetail']['Description'])\n",
    "        return\n",
    "\n",
    "    for codeno in res['BEAAPI']['Results']['ParamValue']:\n",
    "        line_codes.append(codeno['Key'])\n",
    "\n",
    "    # Fetch and process data for each LineCode\n",
    "    for code in line_codes:\n",
    "        f_url = url + f\"?&UserID={api_key}&method=GETDATA&datasetname={dataset_name}&TableName={table_name}&GeoFips=MSA&Year={v_year if v_year else 'ALL'}&LineCode={code}&ResultFormat=JSON\"\n",
    "        res = r.get(f_url).text\n",
    "        res_dict = ast.literal_eval(res)\n",
    "\n",
    "        if 'Error' in res_dict['BEAAPI'].keys():\n",
    "            if res_dict['BEAAPI']['Error']['APIErrorCode'] == 429:\n",
    "                time.sleep(60)\n",
    "            else:\n",
    "                print(\"Error Code: \" + res_dict['BEAAPI']['Error']['APIErrorCode'] + \"\\nError Description: \" + res_dict['BEAAPI']['Error']['ErrorDetail']['Description'])\n",
    "            continue\n",
    "\n",
    "        if 'Data' in res_dict['BEAAPI']['Results']:\n",
    "            df = spark.createDataFrame(res_dict['BEAAPI']['Results']['Data'])\n",
    "\n",
    "            # Add missing columns from STANDARD_COLUMNS\n",
    "            for col in STANDARD_COLUMNS:\n",
    "                if col not in df.columns:\n",
    "                    df = df.withColumn(col, lit(None).cast('string'))\n",
    "\n",
    "            # Reorder DataFrame columns to match STANDARD_COLUMNS\n",
    "            df = df.select(*STANDARD_COLUMNS)\n",
    "\n",
    "            # Union the DataFrame with union_df\n",
    "            if union_df is None:\n",
    "                union_df = df\n",
    "            else:\n",
    "                union_df = union_df.unionByName(df)\n",
    "\n",
    "    if union_df is not None:\n",
    "        union_df.createOrReplaceTempView(\"union_view\")\n",
    "        result_df = spark.sql(\"SELECT DISTINCT Code, TimePeriod FROM union_view\")\n",
    "        result_df.show() \n",
    "\n",
    "# Example usage\n",
    "incremental_load('EB834443-9B97-468D-9EC2-8AAB6D78A922', 'Regional', 'CAGDP1', 2010)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc98b1cf-4763-456d-a7ec-e1b7d6b03c6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "API Test",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
