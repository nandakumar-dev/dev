{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Extract API Parameters\r\n",
        "The goals of this notebook are to\r\n",
        "\r\n",
        "1. Extract the relevant API parameters from the metadata table (ingested in first copy job of Pipeline)\r\n",
        "2. Create list of API call urls to iterate through\r\n",
        "4. Run through list of API calls, ingest data into bronze"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Connect to Blob, Read metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "# This cell will dynamically change to match the parameters passed to it from the Pipeline\r\n",
        "\r\n",
        "run_id = '32fab968-7ad8-432b-a8fe-2fbbf60f441e'\r\n",
        "pipeline_name = 'BEA_Regional'\r\n",
        "source_group = 'BEA'\r\n",
        "metadata_file_name = 'table_linecode_list'\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# import packages\r\n",
        "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\r\n",
        "import pandas as pd\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "import json\r\n",
        "import os\r\n",
        "from io import BytesIO, StringIO\r\n",
        "import numpy as np\r\n",
        "from multiprocessing import Pool\r\n",
        "import tempfile\r\n",
        "import requests\r\n",
        "import time\r\n",
        "import datetime\r\n",
        "from notebookutils import mssparkutils\r\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Get API Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "now = datetime.datetime.now()\r\n",
        "now = now.strftime('%Y-%m-%dT%H:%M:%S')\r\n",
        "api_keys = ['F4993EB8-9C0D-4141-9E37-828C00D09143', '191DA714-6A67-402A-881B-E9F984EA5267', 'A1F7718B-67A6-4597-A30D-AB58AA712921', 'A501A017-5838-4D2C-95E7-B4624C3D2BCF', '930BA9FE-78E2-40FB-A62E-C89ED7DA772F']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "table_linecode_url = 'https://apps.bea.gov/api/data/?UserID=4DB3D6CD-EAD5-482E-BB14-FC7243B611E7&method=GetParameterValuesFiltered&datasetname=Regional&TargetParameter=LineCode'\r\n",
        "response = requests.get(table_linecode_url)\r\n",
        "if response.status_code == 200:\r\n",
        "    metadata_table = json.loads(response.text)\r\n",
        "else:\r\n",
        "    print(f\"Error retrieving data from {table_linecode_url}. Status code: {response.status_code}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# convert parameters to df\r\n",
        "table_array = metadata_table[\"BEAAPI\"][\"Results\"][\"ParamValue\"]\r\n",
        "table_df = pd.DataFrame.from_dict(table_array)\r\n",
        "table_df = table_df.rename(columns = {\"Key\":\"LineCode\"})\r\n",
        "\r\n",
        "# extract table name from desc\r\n",
        "table_df[\"TableName\"] = table_df[\"Desc\"].str.extract('(?<=\\\\[)(.*?)(?=\\\\])') # filter for item between square brackets\r\n",
        "table_df.drop(\"Desc\", axis = 1).drop_duplicates\r\n",
        "table_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# create API calls\r\n",
        "\r\n",
        "# function to reformat blob_path url because it was messing things up in silver\r\n",
        "def reformat_url(url):\r\n",
        "    url = url.replace('https://', '')\r\n",
        "    reserved_characters = ['?']\r\n",
        "    new_url = ''\r\n",
        "    for c in url:\r\n",
        "        if c in reserved_characters:\r\n",
        "            new_url = new_url + '-'\r\n",
        "        else:\r\n",
        "           new_url = new_url + c \r\n",
        "    return(new_url)\r\n",
        "\r\n",
        "state_df = table_df.loc[table_df[\"TableName\"].str.match('^S')].assign(GeoFips = \"STATE\")\r\n",
        "county_df = table_df.loc[table_df[\"TableName\"].str.match('^C')].assign(GeoFips = \"COUNTY\")\r\n",
        "msa_df = table_df.loc[table_df[\"TableName\"].str.match('^[M]')].assign(GeoFips = \"MSA\")\r\n",
        "port_df = table_df.loc[table_df[\"TableName\"].str.match('^P')].assign(GeoFips = \"PORT\")\r\n",
        "\r\n",
        "\r\n",
        "combined_df = pd.concat([state_df, county_df, msa_df, port_df], ignore_index = True)\r\n",
        "combined_df['base_url'] = 'https://apps.bea.gov/api/data/'\r\n",
        "combined_df['api_key'] = pd.qcut(range(len(combined_df)), q = 5, labels = api_keys).to_list()\r\n",
        "combined_df[\"relative_url\"] = \"?UserID=\" + combined_df['api_key'] \\\r\n",
        "    + '&method=GetData&datasetname=Regional&Year=ALL&TableName=' \\\r\n",
        "    + combined_df[\"TableName\"] + '&LineCode=' + combined_df[\"LineCode\"] \\\r\n",
        "    + '&GeoFips=' + combined_df[\"GeoFips\"]\r\n",
        "\r\n",
        "combined_df['full_url'] = combined_df['base_url'] + combined_df['relative_url']\r\n",
        "combined_df['blob_path'] = [reformat_url(u) for u in combined_df['full_url'] + '/' + now + '_' + run_id +'.json']\r\n",
        "\r\n",
        "combined_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Run API calls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# split df into chunks for Multi processing\r\n",
        "\r\n",
        "num_processes = len(api_keys)\r\n",
        "chunks = np.array_split(combined_df, num_processes)\r\n",
        "\r\n",
        "# initiate blob client\r\n",
        "connection_string = \"DefaultEndpointsProtocol=https;AccountName=usafactsbronze;AccountKey=WEH1nIXRgYYjWEjRPC6szld67DOir5Jx46GenOM8bmA+yWQQLlzTeJv5fI02wVxtsW89pSU8lBFc+AStCz7fWw==;EndpointSuffix=core.windows.net\"\r\n",
        "blob_service_client = BlobServiceClient.from_connection_string(connection_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# define function to read blob files\r\n",
        "def ingest_to_bronze(chunk):\r\n",
        "    chunk = chunk.reset_index()\r\n",
        "\r\n",
        "    for i in range(len(chunk)):\r\n",
        "        file_url = chunk['full_url'][i]\r\n",
        "        blob_name = chunk['blob_path'][i]\r\n",
        "\r\n",
        "        try:\r\n",
        "            with tempfile.NamedTemporaryFile(delete=False) as tmp_file:\r\n",
        "                # Download the file from the URL and write it to the temporary file\r\n",
        "                response = requests.get(file_url)\r\n",
        "                tmp_file.write(response.content)\r\n",
        "                tmp_file.flush()\r\n",
        "\r\n",
        "                # Upload the file to Azure Blob Storage\r\n",
        "                blob_client = blob_service_client.get_blob_client('bronze', blob_name)\r\n",
        "                with open(tmp_file.name, \"rb\") as data:\r\n",
        "                    blob_client.upload_blob(data, overwrite = True)\r\n",
        "                \r\n",
        "                # get size in mb\r\n",
        "                file_size = os.path.getsize(tmp_file.name) / (1024 * 1024)\r\n",
        "\r\n",
        "                # Delete the temporary file\r\n",
        "                os.remove(tmp_file.name)\r\n",
        "                print(\"uploaded blob: \" + blob_name)\r\n",
        "                # wait based on file size\r\n",
        "                # side note: It's difficult to not throttle the API key while keeping this process fast. Some files are <1MB while others are 25 MB\r\n",
        "                if file_size < 2:\r\n",
        "                    time.sleep(2)\r\n",
        "                else:\r\n",
        "                    sleep = 60/(100/file_size) + 4\r\n",
        "                    time.sleep(sleep)\r\n",
        "                \r\n",
        "        except requests.exceptions.RequestException as e:\r\n",
        "            print(f\"Error processing {file_url}: {str(e)}\")\r\n",
        "            fail.append(file_url)\r\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# multi processing on API urls, each process uses different API key\r\n",
        "fail = []\r\n",
        "\r\n",
        "with Pool(num_processes) as p:\r\n",
        "    p.map(ingest_to_bronze, chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# add metadata to notebook activity output to check status of URL ingestion\r\n",
        "fail_string = \", \".join(fail)\r\n",
        "mssparkutils.notebook.exit(\"Number of failed urls: \" + str(len(fail)) + \", \" + fail_string)"
      ]
    }
  ]
}