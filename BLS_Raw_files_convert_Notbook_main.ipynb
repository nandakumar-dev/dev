{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 184,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "blob_relative_path = 'download.bls.gov/pub/time.series/sh' \r\n",
        "datetime = '2023-06-06'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from notebookutils import mssparkutils  \r\n",
        "import re\r\n",
        "from pyspark.sql import SparkSession \r\n",
        "from pyspark import SparkContext\r\n",
        "from pyspark.sql.types import StructType,StructField,StringType\r\n",
        "from pyspark.sql.functions import split,col,substring,expr,first,isnull\r\n",
        "# Azure storage access info \r\n",
        "blob_account_name = 'usafactsbronze' # replace with your blob name \r\n",
        "blob_container_name = 'bronze' # replace with your container name  \r\n",
        "linked_service_name = 'bronze' # replace with your linked service name \r\n",
        "\r\n",
        "blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name) \r\n",
        "\r\n",
        "# Allow SPARK to access from Blob remotely \r\n",
        "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path) \r\n",
        "spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token) \r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def get_file_paths(dir_path):\r\n",
        "    file_paths= []\r\n",
        "    files = mssparkutils.fs.ls(dir_path)\r\n",
        "    for file in files:\r\n",
        "        if file.isDir :\r\n",
        "            file_paths.extend(get_file_paths(file.path))\r\n",
        "        else:\r\n",
        "            file_paths.append(file.path)\r\n",
        "\r\n",
        "    return file_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "all_file_paths = get_file_paths(wasbs_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "def try_again(text_path,path):\r\n",
        "\r\n",
        "    lines_rdd = spark.sparkContext.textFile(text_path)\r\n",
        "    lines_rdd = lines_rdd.filter(lambda line: line.strip() != '')\r\n",
        "    values_rdd = lines_rdd.map( lambda line: line.split('\\t')).map(lambda words: [word for word in words] )\r\n",
        "    column_names = values_rdd.first()\r\n",
        "    raw_data = values_rdd.collect()\r\n",
        "\r\n",
        "    data_rdd = values_rdd.filter(lambda line: line != column_names)\r\n",
        "    columns =[re.sub(r'[^A-Za-z0-9]+','_',col.strip()) for col in column_names if col != '']\r\n",
        "    text_data =[]\r\n",
        "    for data in raw_data:\r\n",
        "        if '' in data and len(data) != len(columns):\r\n",
        "            data = list(data)\r\n",
        "            data.remove('')\r\n",
        "            while len(data) > len(columns) and '' in data:\r\n",
        "                data.remove('')\r\n",
        "            text_data.append(data)\r\n",
        "        else:\r\n",
        "            text_data.append(data)\r\n",
        "    data_rdd = sc.parallelize(text_data[1:])\r\n",
        "    if text_path.endswith('.MapErrors'):\r\n",
        "        text_data = raw_data[0]\r\n",
        "        data=[]\r\n",
        "        temp_tuple=[]\r\n",
        "        for value in text_data:\r\n",
        "            if value != '':\r\n",
        "                temp_tuple.append(value)\r\n",
        "            if len(temp_tuple)==5:\r\n",
        "                data.append(tuple(temp_tuple))\r\n",
        "                temp_tuple =[]   \r\n",
        "        columns = data[0]\r\n",
        "    for col in columns :\r\n",
        "        if col.isnumeric():\r\n",
        "            cols = []\r\n",
        "            for ind in range(len(columns)):\r\n",
        "                cols.append('col_'+str(ind+1))\r\n",
        "            data_rdd =values_rdd\r\n",
        "            columns = cols\r\n",
        "    if text_path.endswith('.MapErrors'):\r\n",
        "        df = spark.createDataFrame(data,columns) \r\n",
        "        df.write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"true\").option(\"path\",path).save()\r\n",
        "        return df\r\n",
        "    df = data_rdd.toDF(columns)\r\n",
        "    if df.columns[-1].startswith('_') or len(raw_data[1]) != len(columns):\r\n",
        "        text_data = []\r\n",
        "        if raw_data[2][-1]=='':\r\n",
        "            for text in raw_data :\r\n",
        "                if text[-1]=='':\r\n",
        "                    text_data.append(text[:-1])\r\n",
        "                else:\r\n",
        "                    text_data.append(text)\r\n",
        "            columns =[re.sub(r'[^A-Za-z0-9]+','_',col.strip()) for col in text_data[0]]\r\n",
        "            data = text_data[1:]\r\n",
        "            for col in columns :\r\n",
        "                if col.isnumeric():\r\n",
        "                    cols = []\r\n",
        "                    for ind in range(len(columns)):\r\n",
        "                        cols.append('col_'+str(ind+1))\r\n",
        "                    columns = cols\r\n",
        "                    data = text_data\r\n",
        "                    break\r\n",
        "        else:\r\n",
        "            columns = []\r\n",
        "            \r\n",
        "            for ind in range(len(raw_data[1])):\r\n",
        "                columns.append('col_'+str(ind+1))\r\n",
        "            raw_data_col = raw_data[0]\r\n",
        "            for index in range(len(columns)-len(raw_data[0])):\r\n",
        "                raw_data_col.append(' ')\r\n",
        "            data = []\r\n",
        "            data.append(raw_data_col)\r\n",
        "            data.extend(raw_data[1:])\r\n",
        "        df = spark.createDataFrame(data,columns)\r\n",
        "        \r\n",
        "    df.write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"true\").option(\"path\",path).save()\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def proceed_to_df(text_data):\r\n",
        "   \r\n",
        "    columns =[re.sub(r'[^A-Za-z0-9]+','_',col.strip()) for col in text_data[0] if col != '' ]\r\n",
        "    \r\n",
        "    for col in columns :\r\n",
        "        if col.isnumeric():\r\n",
        "            cols = []\r\n",
        "            for ind in range(len(columns)):\r\n",
        "                cols.append('col_'+str(ind+1))\r\n",
        "            columns = cols\r\n",
        "            df = spark.createDataFrame(text_data,columns)\r\n",
        "            return df\r\n",
        "    df = spark.createDataFrame(text_data[1:],columns)\r\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# text_data = blob_client.download_blob().readall().decode('utf-8').split('\\n')\r\n",
        "# lines_rdd = lines_rdd.split('\\n')\r\n",
        "import re\r\n",
        "dfs = []\r\n",
        "bad_record = []\r\n",
        "bad_records = []\r\n",
        "exclude = ['.exe','.txt','.contacts','.zip','.tar','.xml','.gz','README','.doc','.pdf','.gif','.sf']\r\n",
        "\r\n",
        "for text_path in all_file_paths :\r\n",
        "\r\n",
        "    if not any( excl in text_path.split('/')[-1] for excl in exclude) :\r\n",
        "        if text_path.endswith('.csv'):\r\n",
        "            df = spark.read.csv(text_path,header = True,inferschema = True)\r\n",
        "            df = spark.read.option(\"multiLine\",\"true\").csv(text_path,header = True,inferschema = True)\r\n",
        "            dfs.append((df,text_path))\r\n",
        "            continue\r\n",
        "        try: \r\n",
        "            print(text_path)\r\n",
        "            lines_rdd = spark.sparkContext.textFile(text_path)\r\n",
        "            lines_rdd = lines_rdd.filter(lambda line: line.strip() != '')\r\n",
        "            values_rdd = lines_rdd.map( lambda line: line.split('\\t')).map(lambda words: [tuple(re.split(r'\\s\\s+',word.strip())) for word in words] ) \r\n",
        "            text_data = values_rdd.collect()\r\n",
        "            formatted_data =[]\r\n",
        "            for text in text_data:\r\n",
        "                tuple_data = ()\r\n",
        "                if len(text)>1 : \r\n",
        "                    for word in text:\r\n",
        "                        tuple_data += word\r\n",
        "                    formatted_data.append(tuple_data)\r\n",
        "                else:\r\n",
        "                    formatted_data.append(text[0])       \r\n",
        "            column_names =[col for col in formatted_data[0]]  \r\n",
        "            text_data = []\r\n",
        "            text_data.append(column_names)\r\n",
        "            text_data.extend(formatted_data[1:])\r\n",
        "        except BaseException as e:\r\n",
        "            try:\r\n",
        "                values_rdd = lines_rdd.map( lambda line: line.split('\\t')).map(lambda words: [word for word in words] )\r\n",
        "                column_names = values_rdd.first()\r\n",
        "                data_rdd = values_rdd.filter(lambda line: line != column_names)\r\n",
        "                column_names =[re.sub(r'[^A-Za-z0-9]+','_',col.strip()) for col in column_names if col != '' ]\r\n",
        "                df = data_rdd.toDF(column_names)\r\n",
        "                dfs.append((df,text_path)) \r\n",
        "                continue\r\n",
        "            except BaseException :\r\n",
        "                bad_record.append(text_path)\r\n",
        "                continue\r\n",
        "\r\n",
        "        try:\r\n",
        "            if len(text_data) == 1:\r\n",
        "                schema = StructType([ StructField(re.sub(r'[^A-Za-z0-9]+','_',name.strip()),StringType(),True) for name in text_data[0] if name != ''])\r\n",
        "                df= spark.createDataFrame([],schema)\r\n",
        "                dfs.append((df,text_path))\r\n",
        "                continue\r\n",
        "            if len(column_names) <= 2 :\r\n",
        "                if any(re.match(r'^[- ]*$',line[0]) for line in text_data[:10]) :\r\n",
        "                    for ind,data in enumerate(text_data):\r\n",
        "                        if re.match(r'^[- ]*$',data[0]) :\r\n",
        "                            data =[]\r\n",
        "                            columns = text_data[ind-1]\r\n",
        "                            break\r\n",
        "                \r\n",
        "                    if len(columns) < len(text_data[ind+1]):\r\n",
        "                        columns = [col.split() for col in columns][0]\r\n",
        "\r\n",
        "                    if len(columns) == len(text_data[ind+1]):\r\n",
        "                        data.append(columns)\r\n",
        "                        data.extend(text_data[ind+1:]) \r\n",
        "                        df = proceed_to_df(data)\r\n",
        "                        dfs.append((df,text_path))\r\n",
        "                        continue\r\n",
        "\r\n",
        "            if len(column_names) < len(text_data[1]):\r\n",
        "                if text_data[1][-1] == '':\r\n",
        "                    data =[]\r\n",
        "                    data.append(column_names)\r\n",
        "                    for text in text_data[1:] :\r\n",
        "                        data.append(tuple(list(text)[:-1]))\r\n",
        "                    text_data = data\r\n",
        "                if len(column_names) == len(text_data[1]) :\r\n",
        "                    df = proceed_to_df(text_data)\r\n",
        "                    dfs.append((df,text_path))\r\n",
        "                    continue\r\n",
        "\r\n",
        "                if len(text_data[1]) == len(text_data[2]):\r\n",
        "                    columns=[text.strip() for text in text_data[1] ]\r\n",
        "                    df = spark.createDataFrame(text_data[2:],columns)\r\n",
        "                    new_df = df.drop(col(''))\r\n",
        "                    if len(column_names) == len(new_df.columns):\r\n",
        "                        data=[]\r\n",
        "                        columns=[text.strip() for text in text_data[1] if text !='']\r\n",
        "                        data.append(column_names)\r\n",
        "                        data.append(tuple(columns))\r\n",
        "                        df_data =[tuple(row) for row in new_df.collect()]\r\n",
        "                        data.extend(df_data)\r\n",
        "                        df = proceed_to_df(data)\r\n",
        "                        dfs.append((df,text_path))\r\n",
        "                        continue    \r\n",
        "                    else:\r\n",
        "                        bad_record.append(text_path)\r\n",
        "                        continue\r\n",
        "\r\n",
        "            else:\r\n",
        "                df = proceed_to_df(text_data)\r\n",
        "                dfs.append((df,text_path)) \r\n",
        "                continue \r\n",
        "        except BaseException as e:\r\n",
        "            bad_record.append(text_path)\r\n",
        "            continue\r\n",
        "    else :\r\n",
        "        bad_records.append((text_path,text_path.split('/')[-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "connect_str = \"DefaultEndpointsProtocol=https;AccountName=usafactsbronze;AccountKey=WEH1nIXRgYYjWEjRPC6szld67DOir5Jx46GenOM8bmA+yWQQLlzTeJv5fI02wVxtsW89pSU8lBFc+AStCz7fWw==;EndpointSuffix=core.windows.net\"\r\n",
        "from azure.storage.blob import BlobServiceClient\r\n",
        "container_name='bronze'\r\n",
        "blob_service_client = BlobServiceClient.from_connection_string(connect_str)\r\n",
        "container_client = blob_service_client.get_container_client(container_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from pyspark.sql import SparkSession \r\n",
        "from pyspark.sql.types import * \r\n",
        "from pyspark.sql.functions import split,col,substring,expr,first\r\n",
        "# Azure storage access info \r\n",
        "blob_account_name = 'usafactssilver' # replace with your blob name \r\n",
        "blob_container_name = 'silver' # replace with your container name \r\n",
        "linked_service_name = 'silver' # replace with your linked service name \r\n",
        "\r\n",
        "blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name) \r\n",
        "\r\n",
        "# Allow SPARK to access from Blob remotely \r\n",
        "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path) \r\n",
        "spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "for df in dfs :\r\n",
        "\r\n",
        "    file_location = wasbs_path.split(wasbs_path.split('/')[-2])[0] +'time.series/'+df[1].split('time.series/')[-1]\r\n",
        "    try:\r\n",
        "        df[0].write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"true\").option(\"path\",file_location).save()\r\n",
        "    except BaseException as e:\r\n",
        "        if df[1] not in bad_record:\r\n",
        "            bad_record.append(df[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "for record in bad_record :\r\n",
        "    file_location = wasbs_path.split(wasbs_path.split('/')[-2])[0] +'time.series/'+record.split('time.series/')[-1]\r\n",
        "    try:\r\n",
        "        try_again(record,file_location)\r\n",
        "    except BaseException as e :\r\n",
        "        bad_records.append((record,record.split('/')[-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "if len(bad_records)> 1:\r\n",
        "    pandas_df = pd.DataFrame(bad_records,columns=[\"File_path\",\"File_name\"])\r\n",
        "    file_name = blob_relative_path.split('/')[-1]\r\n",
        "    bad_path = blob_relative_path.split('time.series/')[0]+f'Bad_Records/{file_name}.bad_record.csv'\r\n",
        "    blob_client = container_client.get_blob_client(f\"{bad_path}\")\r\n",
        "    csv_file = pandas_df.to_csv(index=False)\r\n",
        "    blob_client.upload_blob(csv_file,overwrite=True)"
      ]
    }
  ]
}