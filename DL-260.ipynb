{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "blob_relative_path = \"www.macpac.gov/wp-content/uploads/2022/12/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from notebookutils import mssparkutils  \n",
        "from pyspark.sql import SparkSession \n",
        "from pyspark.sql.types import * \n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "from pyspark.sql.functions import *\n",
        "# Azure storage access info \n",
        "blob_account_name = 'usafactsbronze' # replace with your blob name \n",
        "blob_container_name = 'bronze' # replace with your container name \n",
        "linked_service_name = 'bronze' # replace with your linked service name \n",
        "\n",
        "blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name) \n",
        "\n",
        "# Allow SPARK to access from Blob remotely \n",
        "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path) \n",
        "spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token) \n",
        "\n",
        "blob_service_client = BlobServiceClient(account_url=f'https://{blob_account_name}.blob.core.windows.net/', credential=blob_sas_token)\n",
        "container_client = blob_service_client.get_container_client(blob_container_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "pip install xlrd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Azure storage access info \n",
        "blob_account_name = 'usafactssilver' # replace with your blob name \n",
        "blob_container_name = 'silver' # replace with your container name \n",
        "linked_service_name = 'silver' # replace with your linked service name \n",
        "\n",
        "blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name) \n",
        "\n",
        "# Allow SPARK to access from Blob remotely \n",
        "target_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path) \n",
        "spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def get_file_paths(dir_path):\n",
        "    file_paths= []\n",
        "    files = mssparkutils.fs.ls(dir_path)\n",
        "    for file in files:\n",
        "        allowed_extensions = ['.xls', '.xlsx']\n",
        "        if file.isDir :\n",
        "            file_paths.extend(get_file_paths(file.path))\n",
        "        if any(file.path.endswith(ext) for ext in allowed_extensions):\n",
        "            file_paths.append(file.path)\n",
        "\n",
        "    return file_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "all_file_paths = get_file_paths(wasbs_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def concat_xls_row(data,none_count):\n",
        "                \n",
        "    concat_list1 = []\n",
        "    concat_list = []\n",
        "    if none_count == 1:\n",
        "        iter_range = 2\n",
        "    else:\n",
        "        iter_range = none_count\n",
        "    for index in range(iter_range):\n",
        "        if (none_count == 1 and index == 0) or index+1 < none_count :\n",
        "            temp_value = None\n",
        "            temp_list =[]\n",
        "\n",
        "            for value in data[index]:\n",
        "                if value is None:\n",
        "                    value = temp_value\n",
        "                else:\n",
        "                    temp_value = value\n",
        "                temp_list.append(value)\n",
        "            cleaned_row = [value for value in data[index] if value is not None]\n",
        "            if  index == 0 and len(cleaned_row) == 1:\n",
        "                temp_list = [None] + (cleaned_row *(len(data[0])-1))\n",
        "            if len(concat_list1) >= 1:\n",
        "                concat_list1 = [(x if x is not None else ' ') +' '+ (y if y is not None else ' ') for x, y in zip(concat_list1,temp_list)]\n",
        "            else:\n",
        "                concat_list1 =  temp_list\n",
        "        else:\n",
        "            if none_count == 1:\n",
        "                data[0] = concat_list1\n",
        "            else:   \n",
        "                concat_list = [(x if x is not None else ' ') +' '+ (y if y is not None else ' ') for x, y in zip(concat_list1,data[index])]\n",
        "                data[index] = concat_list\n",
        "            \n",
        "            data = data[none_count-1:]\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def convert_list(lst):\n",
        "    counts = {}\n",
        "    converted_list = []\n",
        "    for item in lst:\n",
        "        if item not in counts:\n",
        "            counts[item] = 1\n",
        "            converted_list.append(item)\n",
        "        else:\n",
        "            counts[item] += 1\n",
        "            converted_list.append(item +'_'+ str(counts[item]))\n",
        "\n",
        "    return converted_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def create_df (data,header,footer): \n",
        "    cleaned_columns = [re.sub(r'[^a-zA-Z0-9]', \"_\", value.strip().replace('.0','')) if value is not None else ('col_'+str(ind)) for ind,value in enumerate(data[0])]\n",
        "    columns = cleaned_columns\n",
        "    if len(cleaned_columns) !=  len(list(set(cleaned_columns))):\n",
        "        columns = convert_list(cleaned_columns)\n",
        "    schema = StructType([StructField(name, StringType(), True) for name in columns])\n",
        "    \n",
        "    df = spark.createDataFrame(data[1:], schema)\n",
        "    empty_columns = [column for column in df.columns if df.filter(df[column].isNull()).count() == df.count() ]\n",
        "    df = df.drop(*empty_columns)\n",
        "\n",
        "    if len(df.columns) != len(list(set(df.columns))):\n",
        "        columns = convert_list(cleaned_columns)\n",
        "        df = spark.createDataFrame(data,columns)\n",
        "    if header:\n",
        "        header = '. '.join(header)\n",
        "        df = df.withColumn(\"Header\",lit(header))\n",
        "    if footer:\n",
        "        footer ='. '.join(footer)\n",
        "        df = df.withColumn(\"Footer\",lit(footer))\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def filtered_data(clean_data):\n",
        "    df_data = clean_data\n",
        "    data = []\n",
        "    temp_footer = ''\n",
        "    header= []\n",
        "    footer = []\n",
        "    for ind , row in enumerate(df_data):\n",
        "        row = [None if value is None or value.strip() == '' or value.strip() =='nan'  else value for value in row ]\n",
        "        cleaned_row = [value for value in row if value is not None]\n",
        "        if len(cleaned_row ) > 1 or (len(cleaned_row) == 1 and row[0] is None):\n",
        "            data.append(row)\n",
        "        elif len(data) < 1 :\n",
        "            header += cleaned_row \n",
        "        else:\n",
        "            footer += cleaned_row\n",
        "            \n",
        "\n",
        "    none_count = 0 \n",
        "    \n",
        "    for value in data :\n",
        "        \n",
        "        if value[0] is None or (value.count(None) > 1 and (value[-1] is None and value[-2] is None) ) or (none_count < 1 and value.count(None) >= 1 ) :\n",
        "            none_count += 1\n",
        "        else:\n",
        "            break \n",
        "    \n",
        "    if none_count > 0:\n",
        "        data = concat_xls_row(data,none_count)\n",
        "    level_list = []\n",
        "    level_dict = {}\n",
        "    clean_data = [data[0]]\n",
        "    for ind,row in enumerate(data[1:]):\n",
        "        s = row[1]\n",
        "        if len(s)-len(s.lstrip()) == 0 :\n",
        "            level_dict = {}\n",
        "        level_dict[len(s)-len(s.lstrip())] = s.strip()\n",
        "        row[1] = ' '.join(level_dict.values())\n",
        "        cleaned_row = [value for value in row if value is not None]\n",
        "        if len(cleaned_row) < 2:\n",
        "            pass\n",
        "        else:\n",
        "            clean_data.append(row)\n",
        "            \n",
        "    df = create_df(data,header,footer)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "# Ignore the specific FutureWarning related to iteritems\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"pyspark\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "len(all_file_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from urllib.parse import quote\n",
        "bad_records = []\n",
        "for text_path in all_file_paths:\n",
        "    try:\n",
        "        file_path = text_path.split('.net/')[-1]\n",
        "        file_path = quote(file_path, safe=\"/:\")\n",
        "        file_location = target_path + text_path.split(wasbs_path)[-1]\n",
        "        link = f'https://usafactsbronze.blob.core.windows.net/bronze/{file_path}'\n",
        "        xls = pd.ExcelFile(link)\n",
        "        sheet_dict = pd.read_excel(xls, sheet_name=None ,header = None)\n",
        "        \n",
        "        for sheet_name, sheet_data in sheet_dict.items():\n",
        "            if len(sheet_dict) > 1:\n",
        "                file_location = target_path + text_path.split(wasbs_path)[-1] +'/'+sheet_name\n",
        "            print(file_location)\n",
        "            df = sheet_data\n",
        "            df = df.applymap(lambda x: np.nan if isinstance(x, str) and x.strip()=='' else x)\n",
        "            df = df.dropna(axis=0, how='all').dropna(axis=1, how='all')\n",
        "            df = df.astype(str)\n",
        "            df = spark.createDataFrame(df)\n",
        "            first_row = [value for value in df.first()]\n",
        "            raw_data = df.collect()\n",
        "            if 'nan' in first_row :\n",
        "                df= filtered_data(raw_data)\n",
        "                df.write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"true\").option(\"path\",file_location+'_'+str(ind)).save()\n",
        "            else:\n",
        "                cleaned_columns = [re.sub(r'[^a-zA-Z0-9]', \"_\", value.strip().replace('.0','')) if value is not None else ('col_'+str(ind)) for ind,value in enumerate(raw_data[0])]\n",
        "                df = spark.createDataFrame(raw_data[1:],cleaned_columns)\n",
        "                df = df.select([when(col(c) != 'nan', col(c)).otherwise(None).alias(c) for c in df.columns])\n",
        "                # display(df)\n",
        "                df.write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"true\").option(\"path\",file_location).save()\n",
        "            # print(file_location,'uploaded sucessfully')\n",
        "\n",
        "    except BaseException as e :\n",
        "        bad_records.append((text_path,file_location.split('/')[-1],e))\n",
        "        print(e,file_location)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "len(bad_records)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "if len(bad_records)>= 1:\n",
        "    pandas_df = pd.DataFrame(bad_records,columns=[\"URL\",\"File_name\",\"Reason\"])\n",
        "    bad_path = blob_relative_path+'bad_records/bad_record.csv'\n",
        "    blob_client = container_client.get_blob_client(bad_path)\n",
        "    csv_file = pandas_df.to_csv(index=False)\n",
        "    blob_client.upload_blob(csv_file,overwrite=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
