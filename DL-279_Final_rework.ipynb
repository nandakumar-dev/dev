{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 76,
      "outputs": [],
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "blob_relative_path = \"www.census.gov/housing/hvs/data/histtab7.xlsx\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from notebookutils import mssparkutils  \r\n",
        "from pyspark.sql import SparkSession \r\n",
        "from pyspark.sql.types import * \r\n",
        "import re\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from azure.storage.blob import BlobServiceClient\r\n",
        "from pyspark.sql.functions import *\r\n",
        "# Azure storage access info \r\n",
        "blob_account_name = 'usafactsbronze' # replace with your blob name \r\n",
        "blob_container_name = 'bronze' # replace with your container name \r\n",
        "linked_service_name = 'bronze' # replace with your linked service name \r\n",
        "\r\n",
        "blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name) \r\n",
        "\r\n",
        "# Allow SPARK to access from Blob remotely \r\n",
        "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path) \r\n",
        "spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token) \r\n",
        "\r\n",
        "blob_service_client = BlobServiceClient(account_url=f'https://{blob_account_name}.blob.core.windows.net/', credential=blob_sas_token)\r\n",
        "container_client = blob_service_client.get_container_client(blob_container_name)\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "pip install xlrd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Azure storage access info \r\n",
        "blob_account_name = 'usafactssilver' # replace with your blob name \r\n",
        "blob_container_name = 'silver' # replace with your container name \r\n",
        "linked_service_name = 'silver' # replace with your linked service name \r\n",
        "\r\n",
        "blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name) \r\n",
        "\r\n",
        "# Allow SPARK to access from Blob remotely \r\n",
        "target_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path) \r\n",
        "spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def get_file_paths(dir_path):\r\n",
        "    file_paths= []\r\n",
        "    files = mssparkutils.fs.ls(dir_path)\r\n",
        "    for file in files:\r\n",
        "        allowed_extensions = ['.xls', '.xlsx']\r\n",
        "        if file.isDir :\r\n",
        "            file_paths.extend(get_file_paths(file.path))\r\n",
        "        if any(file.path.endswith(ext) for ext in allowed_extensions):\r\n",
        "            file_paths.append(file.path)\r\n",
        "\r\n",
        "    return file_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "all_file_paths = get_file_paths(wasbs_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def concat_xls_row(data,none_count):\r\n",
        "                \r\n",
        "    concat_list1 = []\r\n",
        "    concat_list = []\r\n",
        "    if none_count == 1:\r\n",
        "        iter_range = 2\r\n",
        "    else:\r\n",
        "        iter_range = none_count\r\n",
        "    for index in range(iter_range):\r\n",
        "        if (none_count == 1 and index == 0) or index+1 < none_count :\r\n",
        "            temp_value = None\r\n",
        "            temp_list =[]\r\n",
        "\r\n",
        "            for value in data[index]:\r\n",
        "                if value is None:\r\n",
        "                    value = temp_value\r\n",
        "                else:\r\n",
        "                    temp_value = value\r\n",
        "                temp_list.append(value)\r\n",
        "            cleaned_row = [value for value in data[index] if value is not None]\r\n",
        "            if  index == 0 and len(cleaned_row) == 1:\r\n",
        "                temp_list = [None] + (cleaned_row *(len(data[0])-1))\r\n",
        "            if len(concat_list1) >= 1:\r\n",
        "                concat_list1 = [(x if x is not None else ' ') +' '+ (y if y is not None else ' ') for x, y in zip(concat_list1,temp_list)]\r\n",
        "            else:\r\n",
        "                concat_list1 =  temp_list\r\n",
        "        else:\r\n",
        "            concat_list = [(x if x is not None else ' ') +' '+ (y if y is not None else ' ') for x, y in zip(concat_list1,data[index])]\r\n",
        "            data[index] = concat_list\r\n",
        "            \r\n",
        "            if none_count == 1 :\r\n",
        "                data = data[index:]\r\n",
        "            else:\r\n",
        "                data = data[none_count-1:]\r\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def create_df (data,header,footer): \r\n",
        "    cleaned_columns = [re.sub(r'[^a-zA-Z0-9]', \"_\", value.strip().replace('.0','')) if value is not None else ('col_'+str(ind)) for ind,value in enumerate(data[0])]\r\n",
        "    columns = []\r\n",
        "\r\n",
        "    if len(cleaned_columns) !=  len(list(set(cleaned_columns))):\r\n",
        "        for ind,column in enumerate(cleaned_columns) :\r\n",
        "            if column not in columns :\r\n",
        "                columns.append(column)\r\n",
        "            else:\r\n",
        "                columns.append(column+'_'+str(ind))\r\n",
        "    else:\r\n",
        "        columns = cleaned_columns\r\n",
        "    schema = StructType([StructField(name, StringType(), True) for name in columns])\r\n",
        "    \r\n",
        "    df = spark.createDataFrame(data[1:], schema)\r\n",
        "    empty_columns = [column for column in df.columns if df.filter(df[column].isNull()).count() == df.count() ]\r\n",
        "    df = df.drop(*empty_columns)\r\n",
        "\r\n",
        "    if len(df.columns) != len(list(set(df.columns))):\r\n",
        "        columns = []\r\n",
        "        for ind,col in enumerate(df.columns):\r\n",
        "            columns.append('col_'+str(ind))\r\n",
        "        df = spark.createDataFrame(data,columns)\r\n",
        "    if header:\r\n",
        "        header = '. '.join(header)\r\n",
        "        df = df.withColumn(\"Header\",lit(header))\r\n",
        "    if footer:\r\n",
        "        footer ='. '.join(footer)\r\n",
        "        df = df.withColumn(\"Footer\",lit(footer))\r\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def seperate_tables(raw_data):\r\n",
        "    datas = []\r\n",
        "    temp_header = []\r\n",
        "    temp_data = []\r\n",
        "    is_header = False\r\n",
        "    for ind, row in enumerate(raw_data):\r\n",
        "        \r\n",
        "        row = [None if value =='nan' or value.strip() == '' else value.replace('……...', '    ').replace('…...', '   ').replace('...',' ').replace('…..',' ').replace('….', '  ').replace('..',' ').replace('  .','').replace('….','').replace('…','') for value in row ]\r\n",
        "        cleaned_row = [value for value in row if value is not None]\r\n",
        "\r\n",
        "        if row[0] is None and len(cleaned_row)==2:\r\n",
        "            row[2:] = [None] * (len(row) - 2)\r\n",
        "        if len(cleaned_row) < 1:\r\n",
        "            continue\r\n",
        "        \r\n",
        "        if ((len(cleaned_row) == 1 and row[0] is None) \r\n",
        "            or (len(raw_data[ind-1])==raw_data[ind-1].count('nan') and len(raw_data[ind-2])==raw_data[ind-2].count('nan') and row[0] is None)\r\n",
        "            or (len(raw_data[ind-1])==raw_data[ind-1].count('nan') and row[0] is None and len(list(set(cleaned_row))) == 1 )\r\n",
        "            or (len(raw_data[ind-1])==raw_data[ind-1].count('nan') and len(cleaned_row) != 1 and raw_data[ind].count('nan')>1 and row[0] is not None and row[2] is None)\r\n",
        "            or (len(raw_data[ind-1])==raw_data[ind-1].count('nan') and row[0] is not None  and row[0].startswith('Revised, based on Vintage'))):\r\n",
        "            if is_header :\r\n",
        "                datas.append(temp_header+temp_data)\r\n",
        "                temp_data = []\r\n",
        "            is_header = True\r\n",
        "        if len(cleaned_row) == 1 and is_header and cleaned_row[0].strip().startswith('Table'):\r\n",
        "            is_header = False\r\n",
        "            datas.append(temp_header+temp_data)\r\n",
        "            \r\n",
        "            temp_data = []\r\n",
        "            temp_header = []\r\n",
        "\r\n",
        "        if is_header :  \r\n",
        "            temp_data.append(row)\r\n",
        "\r\n",
        "        if len(temp_data) < 1 and not is_header :\r\n",
        "            temp_header.append(row)\r\n",
        "        if ind+1 == len(raw_data):\r\n",
        "            datas.append(temp_header+temp_data)\r\n",
        "    return datas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def filtered_data(clean_data):\r\n",
        "    datas = seperate_tables(clean_data)\r\n",
        "    dfs =[]\r\n",
        "    for df_data in datas :\r\n",
        "     \r\n",
        "        data = []\r\n",
        "        temp_footer = ''\r\n",
        "        header= []\r\n",
        "        footer = []\r\n",
        "        for ind , row in enumerate(df_data):\r\n",
        "            cleaned_row = [value for value in row if value is not None]\r\n",
        "            if len(cleaned_row ) > 1 or (len(cleaned_row) == 1 and row[0] is None):\r\n",
        "                if temp_footer :\r\n",
        "                    if row[0] is None:\r\n",
        "                        row[0] = ''\r\n",
        "                    row[0] = temp_footer +'  ' + row[0]\r\n",
        "                data.append(row)\r\n",
        "            elif len(data) < 1 :\r\n",
        "                header += cleaned_row \r\n",
        "            else:\r\n",
        "                if len(cleaned_row) !=0:\r\n",
        "                    temp_footer = cleaned_row[0]\r\n",
        "                    \r\n",
        "                    row = [None if value is None or value.strip() == '' else value for value in df_data[ind-1] ]\r\n",
        "                    prev_clean_row = [value for value in row if value is not None]\r\n",
        "                    if len(prev_clean_row ) < 2   :\r\n",
        "                        footer += cleaned_row\r\n",
        "                    else:\r\n",
        "                        footer = cleaned_row\r\n",
        "\r\n",
        "        none_count = 0 \r\n",
        "        none_exclude = 0\r\n",
        "\r\n",
        "        for value in data :\r\n",
        "            if ((value.count(None) > 1 and None in value[:3]) or (none_count <= 1 and value.count(None) == 1  and value[-1] is not None)  or value[0] is None \r\n",
        "                or all(x.isalpha() if x is not None else True for x in value[1:]) or (value.count(None) == 2 and value[0] is None and value[-1] is None) \r\n",
        "                or (value.count(None) > 4 and value[0] is not None and all(element is None for element in value[1:]))):\r\n",
        "                 \r\n",
        "                none_count += 1\r\n",
        "            else:\r\n",
        "                break \r\n",
        "\r\n",
        "        data = concat_xls_row(data,none_count-1)\r\n",
        "        if len(data) !=0:\r\n",
        "            level_list = []\r\n",
        "            level_dict = {}\r\n",
        "            clean_data = [data[0]]\r\n",
        "            for ind,row in enumerate(data[1:]):\r\n",
        "                s = row[0]\r\n",
        "                if row[0]!=None:\r\n",
        "                    current_key = len(s)-len(s.lstrip())\r\n",
        "                    if current_key == 0 :\r\n",
        "                        level_dict = {}\r\n",
        "                    keys_to_delete = [key for key in level_dict.keys() if key > current_key]\r\n",
        "                    for key in keys_to_delete:\r\n",
        "                        del level_dict[key]\r\n",
        "                    level_dict[current_key] = s.strip()\r\n",
        "                    row[0] = ' '.join(level_dict.values())\r\n",
        "                    cleaned_row = [value for value in row if value is not None]\r\n",
        "                    if len(cleaned_row) < 2:\r\n",
        "                        pass\r\n",
        "                    else:\r\n",
        "                        clean_data.append(row)\r\n",
        "\r\n",
        "        if len(clean_data)>0:\r\n",
        "            df = create_df(clean_data,header,footer)      \r\n",
        "            dfs.append(df)\r\n",
        "    return dfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import warnings\r\n",
        "\r\n",
        "# Ignore the specific FutureWarning related to iteritems\r\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"pyspark\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "from urllib.parse import quote\r\n",
        "bad_records = []\r\n",
        "for text_path in all_file_paths:\r\n",
        "    try:\r\n",
        "        file_path = text_path.split('.net/')[-1]\r\n",
        "        file_path = quote(file_path, safe=\"/:\")\r\n",
        "        file_location = target_path + text_path.split(wasbs_path)[-1]\r\n",
        "        file_location='.'.join(file_location.split('.')[:-1])\r\n",
        "        file_name=file_location.split('/')[-1]\r\n",
        "        file_location=file_location+'/'+file_name\r\n",
        "        link = f'https://usafactsbronze.blob.core.windows.net/bronze/{file_path}'\r\n",
        "        df = pd.read_excel(link,header = None)\r\n",
        "        \r\n",
        "        df = df.applymap(lambda x: np.nan if isinstance(x, str) and x.strip()=='' else x)\r\n",
        "        df = df.dropna(axis=1, how='all') \r\n",
        "        df = df.astype(str)\r\n",
        "        \r\n",
        "        df = spark.createDataFrame(df)\r\n",
        "\r\n",
        "        first_row = [value for value in df.first()]\r\n",
        "        if 'nan' in first_row :\r\n",
        "            raw_data = df.collect()\r\n",
        "            dfs= filtered_data(raw_data)\r\n",
        "            footer_value = []\r\n",
        "            if len(dfs) > 1:\r\n",
        "                last_df = dfs[-1]  \r\n",
        "                footer_value = last_df.select('footer').collect()[0][0]  \r\n",
        "                first_df=dfs[0]\r\n",
        "                Header_value = first_df.select('Header').collect()[0][0]  \r\n",
        "                for ind,df in enumerate(dfs):\r\n",
        "                    df = df.withColumn('Header',lit(Header_value)).withColumn('footer', lit(footer_value)).withColumnRenamed('col_0', \"Description\").withColumnRenamed('', \"Description\")\r\n",
        "                    df.write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"true\").option(\"path\",file_location+'_'+str(ind)).save()\r\n",
        "            else:\r\n",
        "                print(len(dfs))\r\n",
        "                dfs[0].write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"true\").option(\"path\",file_location).save()\r\n",
        "        print(file_location,'uploaded sucessfully')\r\n",
        "\r\n",
        "    except BaseException as e :\r\n",
        "        bad_records1.append((text_path,file_location.split('/')[-1],e))\r\n",
        "        print(e,file_location)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "if len(bad_records)>= 1:\r\n",
        "    pandas_df = pd.DataFrame(bad_records,columns=[\"URL\",\"File_name\",\"Reason\"])\r\n",
        "    bad_path = blob_relative_path+'bad_records/bad_record.csv'\r\n",
        "    blob_client = container_client.get_blob_client(bad_path)\r\n",
        "    csv_file = pandas_df.to_csv(index=False)\r\n",
        "    blob_client.upload_blob(csv_file,overwrite=True)"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}