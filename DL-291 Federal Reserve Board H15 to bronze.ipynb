{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 111,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "pip install lxml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from azure.storage.blob import BlobServiceClient\r\n",
        "import pandas as pd\r\n",
        "import os\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "from lxml import etree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "blob_name='www.federalreserve.gov/datadownload/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "outputs": [],
      "metadata": {},
      "source": [
        "BLOB_ACCOUNT_NAME = 'usafactsbronze'\r\n",
        "LINKED_SERVICE_NAME = 'Bronze'\r\n",
        "container_name='bronze'\r\n",
        "BLOB_SAS_TOKEN = mssparkutils.credentials.getConnectionStringOrCreds(LINKED_SERVICE_NAME)\r\n",
        "# Create a BlobServiceClient\r\n",
        "blob_service_client = BlobServiceClient(\"https://{}.blob.core.windows.net\".format(BLOB_ACCOUNT_NAME), credential=BLOB_SAS_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Azure storage access info \r\n",
        "blob_account_name = 'usafactssilver' # replace with your blob name \r\n",
        "blob_container_name = 'silver' # replace with your container name \r\n",
        "linked_service_name = 'silver' # replace with your linked service name \r\n",
        "\r\n",
        "blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name) \r\n",
        "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/' % (blob_container_name, blob_account_name) \r\n",
        "spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "container_client = blob_service_client.get_container_client(container_name)\r\n",
        "\r\n",
        "# List blobs in the folder\r\n",
        "blobs = container_client.list_blobs(name_starts_with=blob_name)\r\n",
        "\r\n",
        "for blob in blobs:\r\n",
        "    if blob.name.endswith('xml'):\r\n",
        "        # Download the XML file to a local file\r\n",
        "        blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob.name)\r\n",
        "        local_file_path = \"local_copy.xml\"\r\n",
        "        with open(local_file_path, \"wb\") as local_file:\r\n",
        "            blob_data = blob_client.download_blob()\r\n",
        "            blob_data.readinto(local_file)\r\n",
        "\r\n",
        "        # Parse XML file\r\n",
        "        tree = etree.parse(local_file_path)\r\n",
        "        root = tree.getroot()\r\n",
        "        \r\n",
        "        try:\r\n",
        "\r\n",
        "            data_lists = root.xpath('.//frb:DataSet', namespaces=root.nsmap)\r\n",
        "            for data in data_lists:\r\n",
        "                file_id = data.get('id')\r\n",
        "                break\r\n",
        "\r\n",
        "            # Use XPath with a wildcard for the namespace\r\n",
        "            observations = root.xpath('.//*[local-name()=\"Obs\"]')\r\n",
        "\r\n",
        "            # Initialize a list of tuples to store rows of data\r\n",
        "            rows = []\r\n",
        "\r\n",
        "            for obs in observations:\r\n",
        "\r\n",
        "                status = obs.get('OBS_STATUS')\r\n",
        "                time_period = obs.get('TIME_PERIOD')\r\n",
        "                obs_value = float(obs.get('OBS_VALUE'))\r\n",
        "                rows.append((file_id,status,obs_value,time_period))\r\n",
        "\r\n",
        "            columns = ['file_id',\"OBS_STATUS\",\"OBS_VALUE\",\"TIME_PERIOD\", ]\r\n",
        "\r\n",
        "            # Create PySpark DataFrame\r\n",
        "            df = spark.createDataFrame(rows, columns)\r\n",
        "            df.write.format('delta').mode('overwrite').option(\"overwriteSchema\",True).option(\"path\",wasbs_path+blob_name).save()\r\n",
        "            print('file_writen:',blob.name)\r\n",
        "        \r\n",
        "        except:\r\n",
        "            # Extract all CodeList names and descriptions\r\n",
        "            code_lists = root.xpath('.//structure:CodeList', namespaces=root.nsmap)\r\n",
        "\r\n",
        "            data=[]\r\n",
        "            for code_list in code_lists:\r\n",
        "\r\n",
        "                code_list_id = code_list.get('id')\r\n",
        "                code_list_agency =code_list.get('agency')\r\n",
        "                code_list_name = code_list.xpath('./structure:Name', namespaces=root.nsmap)[0].text\r\n",
        "\r\n",
        "                # Extract and print descriptions\r\n",
        "                for code in code_list.xpath('./structure:Code', namespaces=root.nsmap):\r\n",
        "                    code_value = code.get('value')\r\n",
        "                    code_description = code.xpath('./structure:Description', namespaces=root.nsmap)[0].text\r\n",
        "                    data.append((file_id,code_list_agency,code_list_id,code_list_name,code_value,code_description))\r\n",
        "\r\n",
        "            df = spark.createDataFrame(data,schema=['file_id','agency','id','code_list_name','code_value','code_description'])\r\n",
        "            df.write.format('delta').mode('overwrite').option(\"overwriteSchema\",True).option(\"path\",wasbs_path+blob_name).save()\r\n",
        "            print('file_writen:',blob.name)\r\n",
        "\r\n",
        "        # Remove the local file after reading\r\n",
        "        if os.path.exists(local_file_path):\r\n",
        "            os.remove(local_file_path)\r\n",
        "            print(f\"Local file {local_file_path} removed successfully.\")\r\n",
        "        else:\r\n",
        "            print(f\"Local file {local_file_path} does not exist.\")\r\n",
        ""
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}