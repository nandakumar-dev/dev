{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "blob_relative_path = 'https_census.gov_programs-surveys/CENSUS_files/2023-06-01/apes' \r\n",
        "datetime = '2023-06-06'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# %pip install spark_excel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from notebookutils import mssparkutils  \r\n",
        "import re\r\n",
        "# import pyspark_excel\r\n",
        "import pandas as pd\r\n",
        "from pyspark.sql import SparkSession \r\n",
        "from pyspark import SparkContext\r\n",
        "from pyspark.sql.types import StructType,StructField,StringType\r\n",
        "from pyspark.sql.functions import split,col,substring,expr,first,isnull\r\n",
        "# Azure storage access info \r\n",
        "blob_account_name = 'usafactsbronze' # replace with your blob name \r\n",
        "blob_container_name = 'bronze' # replace with your container name  \r\n",
        "linked_service_name = 'bronze' # replace with your linked service name \r\n",
        "\r\n",
        "blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name) \r\n",
        "\r\n",
        "# Allow SPARK to access from Blob remotely \r\n",
        "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path) \r\n",
        "spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token) \r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def get_file_paths(dir_path):\r\n",
        "    file_paths= []\r\n",
        "    files = mssparkutils.fs.ls(dir_path)\r\n",
        "    for file in files:\r\n",
        "        if file.isDir :\r\n",
        "            file_paths.extend(get_file_paths(file.path))\r\n",
        "        else:\r\n",
        "            file_paths.append(file.path)\r\n",
        "\r\n",
        "    return file_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "all_file_paths = get_file_paths(wasbs_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "len(all_file_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# blob_name='download.bls.gov/pub/time.series/oe/oe.data.1.AllData'\r\n",
        "# data=[]\r\n",
        "# blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\r\n",
        "# blob_data = blob_client.download_blob().readall().decode('utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# print(blob_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# text_data = blob_client.download_blob().readall().decode('utf-8')\r\n",
        "# def csv_of_any_length(text_data):\r\n",
        "\r\n",
        "#     data = []\r\n",
        "#     lines = text_data.split('\\n')\r\n",
        "#     header_row = lines[0].split('\\t')\r\n",
        "#     header_length = len(header_row)\r\n",
        "#     max_columns=header_length\r\n",
        "#     maxlength = header_length\r\n",
        "\r\n",
        "#     for idx, line in enumerate(lines):\r\n",
        "#         row = line.split('\\t')\r\n",
        "#         row = [elem.strip() for elem in row]\r\n",
        "\r\n",
        "#         maxlength = max(maxlength, len(row))\r\n",
        "#         row += [''] * (max_columns - len(row)) # fill the row up to max_columns\r\n",
        "\r\n",
        "#         # flag rows that are longer than the header row\r\n",
        "#         if len(row) > header_length:\r\n",
        "#             row.append('CORRUPT_FLAG')\r\n",
        "#             maxlength = max(maxlength, len(row))\r\n",
        "\r\n",
        "#         data.append(row)\r\n",
        "#     max_columns=max(maxlength,max_columns)\r\n",
        "#     for i in range(header_length, max_columns):\r\n",
        "#         header_row.append(f'column{i}')\r\n",
        "#         header_row.append('CORRUPT_FLAG')\r\n",
        "\r\n",
        "#     df = pd.DataFrame(data[1:], columns=header_row)\r\n",
        "#     # fliter df Down to needed\r\n",
        "\r\n",
        "#     filtered_df = df.iloc[:, list(range(maxlength)) + [-1]]\r\n",
        "\r\n",
        "\r\n",
        "#     return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# # import pandas as pd\r\n",
        "# # df =csv_of_any_length(text_data)\r\n",
        "# # display(df)\r\n",
        "# import pandas as pd\r\n",
        "# lines = blob_data.replace('\\r\\n', '\\r').replace('\\n', '\\r').split('\\r') # <-- added to deal with inconsistent formatting of line breaks\r\n",
        "# padding = [[] for _ in range(0, 5) ]\r\n",
        "# # get a trimmed list of headers to parse and map\r\n",
        "# headers=[val.strip() for val in lines[0].split('\\t')]\r\n",
        "# max_len=len(headers)\r\n",
        "# for line in lines:\r\n",
        "#     split_line=line.split('\\t')\r\n",
        "#     max_len=max(max_len,len(split_line))\r\n",
        "#     data.append(line.split('\\t')+padding)\r\n",
        "# df=pd.DataFrame([dat[0:max_len] for dat in data],columns=headers)\r\n",
        "# df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# lines_rdd = spark.sparkContext.textFile(text_path)\r\n",
        "# values_rdd = lines_rdd.map( lambda line: line.split('\\t')).map(lambda words:[word.strip() for word in words])\r\n",
        "# # column_names = values_rdd.first()\r\n",
        "# text = values_rdd.map(tuple).collect()\r\n",
        "# data_rdd = values_rdd.filter(lambda line: line.strip() != column_names)\r\n",
        "# df = data_rdd.toDF(column_names)\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "def try_again(text_path,path):\r\n",
        "\r\n",
        "    lines_rdd = spark.sparkContext.textFile(text_path)\r\n",
        "    lines_rdd = lines_rdd.filter(lambda line: line.strip() != '')\r\n",
        "    values_rdd = lines_rdd.map( lambda line: line.split('\\t')).map(lambda words:[tuple(re.split(r'\\s\\s\\s+',word.strip())) for word in words] )\r\n",
        "    raw_data = values_rdd.collect()\r\n",
        "    formatted_data =[]\r\n",
        "    for text in raw_data:\r\n",
        "        tuple_data = ()\r\n",
        "        if len(text)>1 : \r\n",
        "            for word in text:\r\n",
        "                tuple_data += word\r\n",
        "            formatted_data.append(tuple_data)\r\n",
        "        else:\r\n",
        "            formatted_data.append(text[0])       \r\n",
        "    columns =[col.strip() for col in formatted_data[0]]  \r\n",
        "    text_data = []\r\n",
        "    text_data.append(columns)\r\n",
        "    text_data.extend(formatted_data[1:])\r\n",
        "    if len(text_data[1]) != len(columns):\r\n",
        "        \r\n",
        "        if raw_data[2][-1]=='':\r\n",
        "            text_data = []\r\n",
        "            for text in raw_data :\r\n",
        "                if text[-1]=='':\r\n",
        "                    text_data.append(text[:-1])\r\n",
        "                else:\r\n",
        "                    text_data.append(text)\r\n",
        "            columns =[re.sub(r'[^A-Za-z0-9]+','_',col.strip()) for col in text_data[0]]\r\n",
        "            data = text_data[1:]\r\n",
        "            for col in columns :\r\n",
        "                if col.isnumeric():\r\n",
        "                    cols = []\r\n",
        "                    for ind in range(len(columns)):\r\n",
        "                        cols.append('col_'+str(ind+1))\r\n",
        "                    columns = cols\r\n",
        "                    data = text_data\r\n",
        "                    break\r\n",
        "        else:\r\n",
        "            columns = [] \r\n",
        "\r\n",
        "            for ind in range(len(text_data[1])):\r\n",
        "                columns.append('col_'+str(ind+1))\r\n",
        "            raw_data_col = text_data[0]\r\n",
        "            for index in range(len(columns)-len(text_data[0])):\r\n",
        "                raw_data_col.append(' ')\r\n",
        "            data = []\r\n",
        "            data.append(raw_data_col)\r\n",
        "            data.extend(text_data[1:])\r\n",
        "        df = spark.createDataFrame(data,columns)\r\n",
        "    df.write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"true\").option(\"path\",path).save()\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def proceed_to_df(text_data):\r\n",
        "    columns =[re.sub(r'[^A-Za-z0-9]+','_',col.strip()) for col in text_data[0] if col != '' ]\r\n",
        "    data = text_data[1:]\r\n",
        "    for col in columns :\r\n",
        "        if col.isnumeric():\r\n",
        "            cols = []\r\n",
        "            for ind in range(len(columns)):\r\n",
        "                cols.append('col_'+str(ind+1))\r\n",
        "            columns = cols\r\n",
        "            data =text_data\r\n",
        "            break\r\n",
        "    try:\r\n",
        "        df = spark.createDataFrame(data,columns)\r\n",
        "    except BaseException :\r\n",
        "        new_data = []\r\n",
        "        for text in text_data:\r\n",
        "            data = text\r\n",
        "            if len(text) != len(text_data[0]):\r\n",
        "                temp_data =[]\r\n",
        "                for word in text:\r\n",
        "                    word = word.strip().split(' ')\r\n",
        "                    temp_data.extend(word)\r\n",
        "                data = tuple(temp_data)\r\n",
        "            if len(data) == len(text_data[0]) :\r\n",
        "                new_data.append(tuple(data))\r\n",
        "        df = spark.createDataFrame(new_data,columns)\r\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# text_data = blob_client.download_blob().readall().decode('utf-8').split('\\n')\r\n",
        "# lines_rdd = lines_rdd.split('\\n')\r\n",
        "import re\r\n",
        "dfs = []\r\n",
        "bad_record = []\r\n",
        "bad_records = []\r\n",
        "exclude = ['.exe','.txt','.contacts','.zip','.tar','.xml','.gz','README','.doc','.pdf','.gif','.sf','.xls']\r\n",
        "\r\n",
        "for text_path in all_file_paths:\r\n",
        "\r\n",
        "    if not any( excl in text_path.split('/')[-1] for excl in exclude) :\r\n",
        "        if text_path.endswith('.csv'):\r\n",
        "            df = spark.read.csv(text_path,header = True,inferschema = True)\r\n",
        "            df_pandas = df.toPandas()\r\n",
        "            dfs.append((df_pandas,text_path))\r\n",
        "            continue\r\n",
        "        # if text_path.endswith('.xls'):\r\n",
        "        #     df = spark.read.format('com.crealytics.spark.excel').option('header','true').option('inferschema','true').load(text_path)\r\n",
        "        #     dfs.append((df,text_path))\r\n",
        "        #     continue\r\n",
        "        try: \r\n",
        "            lines_rdd = spark.sparkContext.textFile(text_path)\r\n",
        "            lines_rdd = lines_rdd.filter(lambda line: line.strip() != '')\r\n",
        "            values_rdd = lines_rdd.map( lambda line: line.split('\\t')).map(lambda words: [tuple(re.split(r'\\s+',word.strip())) for word in words] ) \r\n",
        "            text_data = values_rdd.collect()\r\n",
        "            formatted_data =[]\r\n",
        "            for text in text_data:\r\n",
        "                tuple_data = ()\r\n",
        "                if len(text)>1 : \r\n",
        "                    for word in text:\r\n",
        "                        tuple_data += word\r\n",
        "                    formatted_data.append(tuple_data)\r\n",
        "                else:\r\n",
        "                    formatted_data.append(text[0])       \r\n",
        "            column_names =[col for col in formatted_data[0]]  \r\n",
        "            text_data = []\r\n",
        "            text_data.append(column_names)\r\n",
        "            text_data.extend(formatted_data[1:])\r\n",
        "        except BaseException as e:\r\n",
        "            try:\r\n",
        "                values_rdd = lines_rdd.map( lambda line: line.split('\\t')).map(lambda words: [word for word in words] )\r\n",
        "                column_names = values_rdd.first()\r\n",
        "                data_rdd = values_rdd.filter(lambda line: line != column_names)\r\n",
        "                column_names =[re.sub(r'[^A-Za-z0-9]+','_',col.strip()) for col in column_names if col != '' ]\r\n",
        "                df = data_rdd.toDF(column_names)\r\n",
        "                dfs.append((df,text_path)) \r\n",
        "                continue\r\n",
        "            except BaseException :\r\n",
        "                bad_record.append(text_path)\r\n",
        "                continue\r\n",
        "\r\n",
        "        try:\r\n",
        "            if len(text_data) == 1:\r\n",
        "                schema = StructType([ StructField(re.sub(r'[^A-Za-z0-9]+','_',name.strip()),StringType(),True) for name in text_data[0] if name != ''])\r\n",
        "                df= spark.createDataFrame([],schema)\r\n",
        "                dfs.append((df,text_path))\r\n",
        "                continue\r\n",
        "            if len(column_names) <= 2 :\r\n",
        "                if any(re.match(r'^[- ]*$',line[0]) for line in text_data[:10]) :\r\n",
        "                    for ind,data in enumerate(text_data):\r\n",
        "                        if re.match(r'^[- ]*$',data[0]) :\r\n",
        "                            data =[]\r\n",
        "                            columns = text_data[ind-1]\r\n",
        "                            break\r\n",
        "                \r\n",
        "                    if len(columns) < len(text_data[ind+1]):\r\n",
        "                        columns = [col.split() for col in columns][0]\r\n",
        "\r\n",
        "                    if len(columns) == len(text_data[ind+1]):\r\n",
        "                        data.append(columns)\r\n",
        "                        data.extend(text_data[ind+1:]) \r\n",
        "                        df = proceed_to_df(data)\r\n",
        "                        dfs.append((df,text_path))\r\n",
        "                        continue\r\n",
        "\r\n",
        "            if len(column_names) < len(text_data[1]):\r\n",
        "                if text_data[1][-1] == '':\r\n",
        "                    data =[]\r\n",
        "                    data.append(column_names)\r\n",
        "                    for text in text_data[1:] :\r\n",
        "                        data.append(tuple(list(text)[:-1]))\r\n",
        "                    text_data = data\r\n",
        "                if len(column_names) == len(text_data[1]) :\r\n",
        "                    df = proceed_to_df(text_data)\r\n",
        "                    dfs.append((df,text_path))\r\n",
        "                    continue\r\n",
        "\r\n",
        "                if len(text_data[1]) == len(text_data[2]):\r\n",
        "                    columns=[text.strip() for text in text_data[1] ]\r\n",
        "                    df = spark.createDataFrame(text_data[2:],columns)\r\n",
        "                    new_df = df.drop(col(''))\r\n",
        "                    if len(column_names) == len(new_df.columns):\r\n",
        "                        data=[]\r\n",
        "                        columns=[text.strip() for text in text_data[1] if text !='']\r\n",
        "                        data.append(column_names)\r\n",
        "                        data.append(tuple(columns))\r\n",
        "                        df_data =[tuple(row) for row in new_df.collect()]\r\n",
        "                        data.extend(df_data)\r\n",
        "                        df = proceed_to_df(data)\r\n",
        "                        dfs.append((df,text_path))\r\n",
        "                        continue    \r\n",
        "                    else:\r\n",
        "                        bad_record.append(text_path)\r\n",
        "                        continue\r\n",
        "\r\n",
        "            else:\r\n",
        "                df = proceed_to_df(text_data)\r\n",
        "                dfs.append((df,text_path)) \r\n",
        "                continue \r\n",
        "\r\n",
        "        except BaseException as e:\r\n",
        "            bad_record.append(text_path)\r\n",
        "            continue\r\n",
        "    else :\r\n",
        "        bad_records.append((text_path,text_path.split('/')[-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "connect_str = \"DefaultEndpointsProtocol=https;AccountName=usafactsbronze;AccountKey=WEH1nIXRgYYjWEjRPC6szld67DOir5Jx46GenOM8bmA+yWQQLlzTeJv5fI02wVxtsW89pSU8lBFc+AStCz7fWw==;EndpointSuffix=core.windows.net\"\r\n",
        "from azure.storage.blob import BlobServiceClient\r\n",
        "container_name='bronze'\r\n",
        "blob_service_client = BlobServiceClient.from_connection_string(connect_str)\r\n",
        "container_client = blob_service_client.get_container_client(container_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from pyspark.sql import SparkSession \r\n",
        "from pyspark.sql.types import * \r\n",
        "from pyspark.sql.functions import split,col,substring,expr,first\r\n",
        "# Azure storage access info \r\n",
        "blob_account_name = 'usafactssilver' # replace with your blob name \r\n",
        "blob_container_name = 'silver' # replace with your container name \r\n",
        "linked_service_name = 'silver' # replace with your linked service name \r\n",
        "# blob_relative_path = 'https_census.gov_programs-surveys/CENSUS_files/2023-06-01/apes'\r\n",
        "blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name) \r\n",
        "\r\n",
        "# Allow SPARK to access from Blob remotely \r\n",
        "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path) \r\n",
        "spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# for file in files:\r\n",
        "#     print(file.path)\r\n",
        "#     df = spark.read.format(\"delta\").load(file.path)\r\n",
        "#     display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "for df in dfs :\r\n",
        "\r\n",
        "    file_location = wasbs_path.split(wasbs_path.split('/')[-2])[0]+df[1].split('CENSUS_files/')[1]\r\n",
        "    try:\r\n",
        "        df[0].write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"true\").option(\"path\",file_location).save()\r\n",
        "    except BaseException as e:\r\n",
        "        if df[1] not in bad_record:\r\n",
        "            bad_record.append(df[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "for record in bad_record:\r\n",
        "    file_location = wasbs_path.split(wasbs_path.split('/')[-2])[0]+record.split('CENSUS_files/')[1]\r\n",
        "    try:\r\n",
        "        try_again(record,file_location)\r\n",
        "    except BaseException as e :\r\n",
        "        print(e)\r\n",
        "        # bad_records.append((record,record.split('/')[-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "if len(bad_records)> 1:\r\n",
        "    pandas_df = pd.DataFrame(bad_records,columns=[\"File_path\",\"File_name\"])\r\n",
        "    file_name = blob_relative_path.split('/')[-1]\r\n",
        "    bad_path = blob_relative_path.split(blob_relative_path.split('/')[-2])[0]+f'Bad_Records/{datetime}/{file_name}.bad_record.csv'\r\n",
        "    blob_client = container_client.get_blob_client(f\"{bad_path}\")\r\n",
        "    csv_file = pandas_df.to_csv(index=False)\r\n",
        "    blob_client.upload_blob(csv_file,overwrite=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "print(bad_records)"
      ]
    }
  ]
}