{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 50,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from pyspark.sql import SparkSession\r\n",
        "from pyspark.sql.functions import *\r\n",
        "from pyspark.sql.types import *\r\n",
        "import pandas as pd\r\n",
        "from azure.storage.blob import BlobServiceClient\r\n",
        "import tempfile\r\n",
        "import json\r\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "folder_path='cde.ucr.cjis.gov/LATEST/participation/state/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Blob Connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Create a connection to the Blob storage account\r\n",
        "container_name = \"bronze\"\r\n",
        "BLOB_ACCOUNT_NAME = 'usafactsbronze'\r\n",
        "LINKED_SERVICE_NAME = 'Bronze'\r\n",
        "BLOB_SAS_TOKEN = mssparkutils.credentials.getConnectionStringOrCreds(LINKED_SERVICE_NAME)\r\n",
        "blob_service_client = BlobServiceClient(\"https://{}.blob.core.windows.net\".format(BLOB_ACCOUNT_NAME), credential=BLOB_SAS_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Azure storage access info \r\n",
        "blob_account_name = 'usafactssilver' # replace with your blob name \r\n",
        "blob_container_name = 'silver' # replace with your container name \r\n",
        "linked_service_name = 'silver' # replace with your linked service name \r\n",
        "\r\n",
        "blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name) \r\n",
        "\r\n",
        "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/' % (blob_container_name, blob_account_name) \r\n",
        "spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token) \r\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Extracting directory name from the blob's name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "directories = set()\r\n",
        "container_client = blob_service_client.get_container_client(container_name)\r\n",
        "\r\n",
        "try:\r\n",
        "    # List all blobs in the container\r\n",
        "    blobs = container_client.list_blobs(name_starts_with=folder_path)\r\n",
        "\r\n",
        "    for blob in blobs:\r\n",
        "        # Extract the directory name from the blob's name\r\n",
        "        directory_name = '/'.join(blob.name.split('/')[:-1])\r\n",
        "\r\n",
        "        # Add the directory to the set\r\n",
        "        directories.add(directory_name)\r\n",
        "\r\n",
        "except Exception as e:\r\n",
        "    print(f\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Converting Json to delta tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "for folder_name in directories:\r\n",
        "    \r\n",
        "    container_client = blob_service_client.get_container_client(container_name)\r\n",
        "\r\n",
        "    # List all blobs in the container with the specified prefix\r\n",
        "    blobs = container_client.list_blobs(name_starts_with=folder_name)\r\n",
        "    dataframes = []\r\n",
        "    error_list=[]\r\n",
        "\r\n",
        "    # Read each JSON file individually and write separately\r\n",
        "    for blob in blobs:\r\n",
        "        try:\r\n",
        "            blob_client = container_client.get_blob_client(blob.name)\r\n",
        "            content = blob_client.download_blob().readall()\r\n",
        "            json_data = json.loads(content)\r\n",
        "\r\n",
        "            # Check if the JSON is nested or not\r\n",
        "            if isinstance(json_data, dict) and len(json_data) > 0 :\r\n",
        "                # Extract column names dynamically from JSON keys\r\n",
        "                columns = [\"data_year\"] + [key.replace(\" \", \"_\").replace(',','_').replace('(','').replace(')','') for key in json_data[\"keys\"]]\r\n",
        "\r\n",
        "                # Define the schema dynamically\r\n",
        "                schema = StructType([StructField(col, IntegerType(), True) for col in columns])\r\n",
        "\r\n",
        "                # Create a DataFrame from the JSON data with the dynamic schema and values\r\n",
        "                data = [(row[\"data_year\"], *[row[key] for key in json_data[\"keys\"]]) for row in json_data[\"data\"]]\r\n",
        "\r\n",
        "                df = spark.createDataFrame(data, schema)\r\n",
        "                df=df.orderBy('data_year')\r\n",
        "                df=df.coalesce(1)\r\n",
        "                silver_blob_relative_path1=blob.name\r\n",
        "                wasbs_path1=wasbs_path+silver_blob_relative_path1\r\n",
        "                df.write.format('delta').mode('overwrite').option(\"overwriteSchema\",True).option(\"path\",wasbs_path1).save()\r\n",
        "                print('file_uploaded 1:', blob.name)\r\n",
        "        \r\n",
        "\r\n",
        "            elif isinstance(json_data, list) and len(json_data) > 0:\r\n",
        "                # Directly create DataFrame from the JSON data\r\n",
        "                df = spark.read.option(\"inferSchema\", \"true\").json(spark.sparkContext.parallelize([json_data]))\r\n",
        "                # Append the Dataframes to the empty list\r\n",
        "                dataframes.append(df)\r\n",
        "\r\n",
        "            elif isinstance(json_data, list) and len(json_data) == 0:\r\n",
        "                print('No_json_data_found:', blob.name)\r\n",
        "            \r\n",
        "            else:\r\n",
        "                print('Not_a_JSON_file:', blob.name)\r\n",
        "\r\n",
        "        except Exception as e:\r\n",
        "\r\n",
        "            error_list.append((wasbs_path, str(e)))\r\n",
        "            print('error_found')\r\n",
        "\r\n",
        "    if len(dataframes) > 0:\r\n",
        "\r\n",
        "        result_df = dataframes[0]\r\n",
        "        for df in dataframes[1:]:\r\n",
        "            if len(df.columns)==len(result_df.columns):\r\n",
        "                result_df = result_df.union(df)\r\n",
        "            else:\r\n",
        "                columns_df1 = result_df.columns\r\n",
        "                columns_df2 = df.columns\r\n",
        "\r\n",
        "                # Find the columns that are missing in each dataframe\r\n",
        "                missing_columns_df1 = [col for col in columns_df2 if col not in columns_df1]\r\n",
        "                missing_columns_df2 = [col for col in columns_df1 if col not in columns_df2]\r\n",
        "\r\n",
        "                # Add missing columns with null values\r\n",
        "                for col in missing_columns_df1:\r\n",
        "                    result_df = result_df.withColumn(col, lit(None))\r\n",
        "\r\n",
        "                for col in missing_columns_df2:\r\n",
        "                    df = df.withColumn(col, lit(None))\r\n",
        "\r\n",
        "                # Select columns in the same order\r\n",
        "                df = df.select(result_df.columns)\r\n",
        "\r\n",
        "                # Use union to merge the dataframes\r\n",
        "                result_df = result_df.union(df)\r\n",
        "\r\n",
        "        result_df=result_df.orderBy('data_year')\r\n",
        "        result_df=result_df.coalesce(1)\r\n",
        "        # Writing in Silver Layer\r\n",
        "        silver_blob_relative_path2 = folder_name\r\n",
        "\r\n",
        "        wasbs_path2=wasbs_path+silver_blob_relative_path2\r\n",
        "\r\n",
        "        result_df.write.format('delta').mode('overwrite').option(\"overwriteSchema\",True).option(\"path\",wasbs_path2).save()\r\n",
        "        print('file_uploaded 2:', silver_blob_relative_path2)\r\n",
        "\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Uploading error log to blob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 291,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "if len(error_list) !=0:\r\n",
        "    pandas_df = pd.DataFrame(error_list,columns=[\"URL\",\"Reason\"])\r\n",
        "    filelocation = f'{folder_name}'+'error_files'+'/'+ f\"error.csv\"\r\n",
        "    blob_client = blob_service_client.get_blob_client(container_name,f\"{filelocation}\")\r\n",
        "    csv_file = pandas_df.to_csv(index=False)\r\n",
        "    blob_client.upload_blob(csv_file,overwrite=True)\r\n",
        "    print('error_file_uploaded',filelocation)"
      ]
    }
  ]
}