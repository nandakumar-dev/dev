{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 32,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import pandas as pd \r\n",
        "import json\r\n",
        "from azure.storage.blob import BlobServiceClient\r\n",
        "from pyspark.sql.types import *\r\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "blob_path='data.medicaid.gov/api/1/datastore/query/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "BLOB_ACCOUNT_NAME = 'usafactsbronze'\r\n",
        "LINKED_SERVICE_NAME = 'bronze'\r\n",
        "input_container_name='bronze'\r\n",
        "BLOB_SAS_TOKEN = mssparkutils.credentials.getConnectionStringOrCreds(LINKED_SERVICE_NAME)\r\n",
        "source_blob_service_client = BlobServiceClient(\"https://{}.blob.core.windows.net\".format(BLOB_ACCOUNT_NAME), credential=BLOB_SAS_TOKEN)\r\n",
        "container_client = source_blob_service_client.get_container_client(input_container_name)\r\n",
        "blobs = container_client.list_blobs(name_starts_with=blob_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Azure storage access info \r\n",
        "blob_account_name = 'usafactssilver'\r\n",
        "blob_container_name = 'silver'\r\n",
        "linked_service_name = 'silver' \r\n",
        "blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name) \r\n",
        "# Allow SPARK to access from Blob remotely\r\n",
        "spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "for blob in blobs:\r\n",
        "    try:\r\n",
        "        blob_client = container_client.get_blob_client(blob) \r\n",
        "        blob_data = blob_client.download_blob().readall()\r\n",
        "        json_data = json.loads(blob_data)\r\n",
        "        table_data=json_data['results']\r\n",
        "        df=pd.DataFrame(table_data)\r\n",
        "        spark_schema = StructType([StructField(col, StringType(), True) for col in df.columns])\r\n",
        "        spark_df = spark.createDataFrame(df, schema=spark_schema)\r\n",
        "        special_characters = r'[^a-zA-Z0-9-_/.]'\r\n",
        "        file_path = re.sub(special_characters, '_', blob.name)\r\n",
        "        wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name,file_path) \r\n",
        "        spark_df.write.format('delta').mode('overwrite').option(\"overwriteSchema\",True).option(\"path\",wasbs_path).save()\r\n",
        "        print('file_uploaded:',file_path)\r\n",
        "    except Exception as e:\r\n",
        "        print(f'error:{blob.name}',str(e))"
      ]
    }
  ]
}