{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from pyspark.sql import SparkSession \r\n",
        "from pyspark.sql.types import * \r\n",
        "from pyspark.sql.functions import * \r\n",
        "import zipfile\r\n",
        "import pandas as pd\r\n",
        "from azure.storage.blob import BlobServiceClient, BlobClient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "file_path='CENSUS/supp folder file and codebook path.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Azure storage access info \r\n",
        "blob_account_name = 'usafactsbronze' # replace with your blob name \r\n",
        "blob_container_name = 'ingestion-meta' # replace with your container name  \r\n",
        "linked_service_name = 'bronze' # replace with your linked service name \r\n",
        "\r\n",
        "blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name)\r\n",
        "# Allow SPARK to access from Blob remotely \r\n",
        "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, file_path) \r\n",
        "spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token) \r\n",
        "\r\n",
        "url_df = spark.read.csv(wasbs_path,header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def read_df(blob_path):\r\n",
        "    # Azure storage access info \r\n",
        "    blob_account_name = 'usafactsbronze' # replace with your blob name \r\n",
        "    blob_container_name = 'bronze' # replace with your container name  \r\n",
        "    linked_service_name = 'bronze' # replace with your linked service name \r\n",
        "\r\n",
        "    blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name)\r\n",
        "    # Allow SPARK to access from Blob remotely \r\n",
        "    wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_path) \r\n",
        "    spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token) \r\n",
        "    return wasbs_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "creating a list of tuples that will be used to split the columns of a dataset based on their index positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def tuples_list(code_book_df):\r\n",
        "    df_no_duplicates =code_book_df.dropDuplicates([\"Start\", \"Size\",\"Name\"]) \r\n",
        "    selected_columns = [\"Start\", \"Size\",\"Name\"]\r\n",
        "    list_of_tuples = df_no_duplicates.select(selected_columns).rdd.map(tuple).collect()\r\n",
        "    split_columns=[]\r\n",
        "    for Tape_start,length,Field_name in list_of_tuples :\r\n",
        "\r\n",
        "        column_name = 'value'\r\n",
        "\r\n",
        "        split_columns.append(expr(f\"substring({column_name}, {Tape_start},{length})\").alias(Field_name))\r\n",
        "    return split_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# silver storage access\r\n",
        "# Azure storage access info \r\n",
        "blob_account_name = 'usafactssilver'\r\n",
        "blob_container_name = 'silver'\r\n",
        "linked_service_name = 'silver' \r\n",
        "\r\n",
        "blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name) \r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "for file_path,codebook_path in url_df.rdd.collect():\r\n",
        "    txt_file_blob_path = file_path\r\n",
        "    code_book_blob_path = codebook_path\r\n",
        "    file_name=txt_file_blob_path.split(\"/\")[-1].split(\".\")[0]\r\n",
        "    folder_path='/'.join(txt_file_blob_path.split(\"/\")[:-2])\r\n",
        "\r\n",
        "    output_blob_path1= f'{folder_path}/{file_name}'\r\n",
        "\r\n",
        "    input_txt_file_df = spark.read.text(read_df(txt_file_blob_path))\r\n",
        "\r\n",
        "    code_book_df=spark.read.option(\"multiLine\",\"true\").csv(read_df(code_book_blob_path),header=True)\r\n",
        "\r\n",
        "    code_book_df = code_book_df.withColumn(\"Name\", regexp_replace(col(\"Name\"), \" \", \"_\"))\r\n",
        "    code_book_df=code_book_df.dropDuplicates()\r\n",
        "    code_book_df =code_book_df.dropDuplicates([\"Name\",\"Code\"])\r\n",
        "\r\n",
        "    input_txt_file_df = input_txt_file_df.select(\"*\",*tuples_list(code_book_df)).drop('value')\r\n",
        "\r\n",
        "    # Allow SPARK to access from Blob remotely\r\n",
        "    wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, output_blob_path1)\r\n",
        "    spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token)\r\n",
        "    \r\n",
        "    input_txt_file_df.write.format('delta').mode('overwrite').option(\"overwriteSchema\",True).option(\"path\",wasbs_path).save()\r\n",
        "    print('file_written: ', output_blob_path1)\r\n",
        ""
      ]
    }
  ]
}