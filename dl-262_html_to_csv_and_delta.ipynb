{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 63,
      "outputs": [],
      "metadata": {},
      "source": [
        "file_path='www.ssa.gov/www_ssa_gov_oact_ssir_SSI20_SingleYearTables_ssiSingleYearIndex_html.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\r\n",
        "import pandas as pd\r\n",
        "from io import StringIO\r\n",
        "from pyspark.sql.functions import *\r\n",
        "import requests\r\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "linked_service_name = 'bronze'\r\n",
        "container_name = 'ingestion-meta'\r\n",
        "account_name = 'usafactsbronze'\r\n",
        "blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name)\r\n",
        "blob_service_client = BlobServiceClient(account_url=f'https://{account_name}.blob.core.windows.net/', credential=blob_sas_token)\r\n",
        "container_client = blob_service_client.get_container_client(container_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def write_bronze(df,pdf_path):\r\n",
        "\r\n",
        "    BLOB_ACCOUNT_NAME = 'usafactsbronze'\r\n",
        "    LINKED_SERVICE_NAME = 'Bronze'\r\n",
        "    BLOB_SAS_TOKEN = mssparkutils.credentials.getConnectionStringOrCreds(LINKED_SERVICE_NAME)\r\n",
        "    blob_service_client = BlobServiceClient(\"https://{}.blob.core.windows.net\".format(BLOB_ACCOUNT_NAME), credential=BLOB_SAS_TOKEN)\r\n",
        "    container_name='bronze'\r\n",
        "\r\n",
        "    ouput_file_path=pdf_path.replace(\"https://\",\"\").split('.')[:-1]\r\n",
        "    ouput_file_path='.'.join(ouput_file_path)+'.csv'\r\n",
        "\r\n",
        "    csv_data=df.to_csv(index=False)\r\n",
        "\r\n",
        "    # Create a blob client for the new file\r\n",
        "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=ouput_file_path)\r\n",
        "\r\n",
        "    # Upload the CSV data to the blob\r\n",
        "    blob_client.upload_blob(csv_data, overwrite=True)\r\n",
        "\r\n",
        "    print(\"CSV file uploaded successfully:\",ouput_file_path)\r\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def write_silver(df,ouput_file_path):    \r\n",
        "    # silver storage access\r\n",
        "    # Azure storage access info \r\n",
        "    blob_account_name = 'usafactssilver'\r\n",
        "    blob_container_name = 'silver'\r\n",
        "    linked_service_name = 'silver' \r\n",
        "\r\n",
        "    blob_sas_token = mssparkutils.credentials.getConnectionStringOrCreds(linked_service_name) \r\n",
        "\r\n",
        "    ouput_file_path=ouput_file_path.replace('https://','')\r\n",
        "\r\n",
        "    # Allow SPARK to access from Blob remotely\r\n",
        "    wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, ouput_file_path)\r\n",
        "    spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token)\r\n",
        "    df.write.format('delta').mode('overwrite').option(\"overwriteSchema\",True).option(\"path\",wasbs_path).save()\r\n",
        "    print('DELTA file uploaded successfully: ',ouput_file_path)\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "blob_client = container_client.get_blob_client(file_path)\r\n",
        "content = blob_client.download_blob().readall()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# Convert bytes to string using StringIO\r\n",
        "blob_string = str(content, 'utf-8')\r\n",
        "blob_csv = StringIO(blob_string)\r\n",
        "\r\n",
        "# Create Pandas DataFrame\r\n",
        "df = pd.read_csv(blob_csv)\r\n",
        "\r\n",
        "\r\n",
        "# Create a list from the specified column\r\n",
        "url_list = df.URL.tolist()\r\n",
        "\r\n",
        "for url in url_list:\r\n",
        "    Header=[] \r\n",
        "    Footer=[]\r\n",
        "    # Sending HTTP GET request to the URL\r\n",
        "    response = requests.get(url)\r\n",
        "\r\n",
        "    # Check if the request was successful (status code 200)\r\n",
        "    if response.status_code == 200:\r\n",
        "        # Get the HTML content from the response\r\n",
        "        html_content = response.text\r\n",
        "        # Parse the HTML content with BeautifulSoup\r\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\r\n",
        "\r\n",
        "        url_links = [a for a in soup.find_all(\"div\")]\r\n",
        "        # print(url_links)\r\n",
        "        header= soup.find('div', {'class': 'fs2 fw6 ta-c'}).get_text(strip=True)\r\n",
        "\r\n",
        "        # Find all div tags with class \"fs2 fw6 ta-c\"\r\n",
        "        div_tags = soup.find_all('div', {'class': 'bt'})\r\n",
        "\r\n",
        "        # Extract and print the text content within each div tag\r\n",
        "        for div_tag in div_tags:\r\n",
        "            header_text = div_tag.get_text(strip=True)\r\n",
        "            footers = header_text.split('\\n')\r\n",
        "            for footer in footers:\r\n",
        "                Footer.append(footer.strip())\r\n",
        "            \r\n",
        "        div_tags = soup.find_all('div', {'class': 'pb2'})\r\n",
        "\r\n",
        "        # Extract and print the text content within each div tag\r\n",
        "        for div_tag in div_tags:\r\n",
        "            header_text = div_tag.get_text(strip=True)\r\n",
        "            footers = header_text.split('\\n')\r\n",
        "            for footer in footers:\r\n",
        "                Footer.append(footer.strip())\r\n",
        "\r\n",
        "        div_tags = soup.find_all('li')\r\n",
        "        if div_tags:\r\n",
        "            Footer.append('Notes:')\r\n",
        "        # Extract and print the text content within each div tag\r\n",
        "        for num,div_tag in enumerate(div_tags,start=1):\r\n",
        "            header_text = div_tag.get_text(strip=True)\r\n",
        "            footers = header_text.split('\\n')\r\n",
        "            footers[0]=str(num)+'.' +footers[0].strip()\r\n",
        "            for footer in footers:\r\n",
        "                Footer.append(footer.strip())\r\n",
        "\r\n",
        "        # header_text = soup.find('div', {'class': 'cell print-dn w-100'}).get_text(strip=True)\r\n",
        "        # print(header_text)\r\n",
        "        notes = [a.get_text(strip=True) for a in soup.find_all('td')]\r\n",
        "        if notes[-1].startswith('Note'):\r\n",
        "            Footer.append(notes[-1])\r\n",
        "\r\n",
        "    footer=' '.join(Footer)\r\n",
        "    # header = header.split('\\n')\r\n",
        "\r\n",
        "    try:\r\n",
        "        pandas_df=pd.read_html(url)\r\n",
        "        pandas_df = pandas_df[1]\r\n",
        "\r\n",
        "        pandas_df = pandas_df.dropna(axis=1)\r\n",
        "\r\n",
        "\r\n",
        "        # write_bronze(pandas_df,url)\r\n",
        "\r\n",
        "        spark_df=spark.createDataFrame(pandas_df)\r\n",
        "        \r\n",
        "        columns = spark_df.columns\r\n",
        "        new_columns = [col(c).alias(c.replace(\" \", \"_\").replace(\",\", \"_\").replace(\";\", \"_\")\r\n",
        "                            .replace(\"{\", \"_\").replace(\"}\", \"_\")\r\n",
        "                            .replace(\"(\", \"_\").replace(\")\", \"_\")\r\n",
        "                            .replace(\"\\n\", \"_\").replace(\"\\t\", \"_\").replace(\"=\", \"_\"))\r\n",
        "                for c in spark_df.columns]\r\n",
        "\r\n",
        "        # Apply the new column names to the DataFrame\r\n",
        "        spark_df = spark_df.select(*new_columns)\r\n",
        "        spark_df = spark_df.drop(*[col_name for col_name in spark_df.columns if spark_df.filter(col(col_name).isNotNull()).count() == 0])\r\n",
        "        condition = ~((col(columns[0]) == 'Historical data:') | (col(columns[0]) == 'Projected:'))\r\n",
        "        for column in columns:\r\n",
        "            condition &= ~((col(column) == 'Historical data:') | (col(column) == 'Projected:'))\r\n",
        "\r\n",
        "        spark_df = spark_df.filter(condition)\r\n",
        "        spark_df=spark_df.withColumn('Header',lit(header)).withColumn('Footer',lit(footer))\r\n",
        "        spark_df=spark_df.coalesce(1)\r\n",
        "        # display(spark_df)\r\n",
        "        write_silver(spark_df,url)\r\n",
        "\r\n",
        "    except:\r\n",
        "\r\n",
        "        try:\r\n",
        "            pandas_df=pd.read_html(url)\r\n",
        "            pandas_df = pandas_df[1]\r\n",
        "            pandas_df = pandas_df.dropna(axis=1,how='all')\r\n",
        "\r\n",
        "            # write_bronze(pandas_df,url)\r\n",
        "\r\n",
        "            spark_df=spark.createDataFrame(pandas_df)\r\n",
        "            \r\n",
        "            columns = spark_df.columns\r\n",
        "            new_columns = [col(c).alias(c.replace(\" \", \"_\").replace(\",\", \"_\").replace(\";\", \"_\")\r\n",
        "                                .replace(\"{\", \"_\").replace(\"}\", \"_\")\r\n",
        "                                .replace(\"(\", \"_\").replace(\")\", \"_\")\r\n",
        "                                .replace(\"\\n\", \"_\").replace(\"\\t\", \"_\").replace(\"=\", \"_\"))\r\n",
        "                    for c in spark_df.columns]\r\n",
        "\r\n",
        "            # Apply the new column names to the DataFrame\r\n",
        "            spark_df = spark_df.select(*new_columns)\r\n",
        "            spark_df = spark_df.drop(*[col_name for col_name in spark_df.columns if spark_df.filter(col(col_name).isNotNull()).count() == 0])\r\n",
        "            condition = ~((col(columns[0]) == 'Historical data:') | (col(columns[0]) == 'Projected:'))\r\n",
        "            for column in columns:\r\n",
        "                condition &= ~((col(column) == 'Historical data:') | (col(column) == 'Projected:'))\r\n",
        "            spark_df = spark_df.filter(condition)\r\n",
        "            spark_df=spark_df.withColumn('Header',lit(header)).withColumn('Footer',lit(footer))\r\n",
        "            spark_df=spark_df.coalesce(1)\r\n",
        "            # display(spark_df)\r\n",
        "            write_silver(spark_df,url)\r\n",
        "\r\n",
        "        except Exception as e:\r\n",
        "            print(url,str(e))\r\n",
        "    \r\n",
        "    \r\n",
        ""
      ]
    }
  ]
}